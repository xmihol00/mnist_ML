{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2f6df1",
   "metadata": {},
   "source": [
    "# Neural network for the MNIST dataset\n",
    "The MNIST dataset is a dataset of handwritten digits. Each digit is a 28x28 monochrome picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6aa62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "\n",
    "L1_SIZE = 56\n",
    "L2_SIZE = 28\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f253a",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffc689fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aacbd",
   "metadata": {},
   "source": [
    "### Non linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35d68cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, L)\n",
    "\n",
    "def ReLU_deriv(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return L > 0\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :L: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    return np.exp(L) / sum(np.exp(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb692aa",
   "metadata": {},
   "source": [
    "### Initialization of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "565c9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_2L(size: int, input_size = PIXELS_PER_IMAGE, output_size = CLASSES_COUNT) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :size: Number of neurons in a hidden layer.\n",
    "    :input_size: Number of input values.\n",
    "    :output_size: Number of neurons in a hidden layer an output layer.\n",
    "    :return: Tuple of randomly initilized weight matrices real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The order of matrix multiplication by weight matrices is given by the arrangement of weights into columns and rows, see below and in other functions.\n",
    "    W1 = np.random.rand(size, input_size) - 0.5  # Each column is applied to respective input value, weights for each neuron of the first hidden layer are in rows.\n",
    "    W2 = np.random.rand(output_size, size) - 0.5 # Each column is applied to respective neuron of the first hidden layer, weights for each neuron of the output layer are in rows.\n",
    "    return W1, W2\n",
    "\n",
    "def weights_init_3L(l1_size: int, l2_size: int, input_size = PIXELS_PER_IMAGE, output_size = CLASSES_COUNT) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :l1_size: Number of neurons in a first hidden layer.\n",
    "    :l2_size: Number of neurons in a second hidden layer.\n",
    "    :input_size: Number of input values.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of randomly initilized weight matrices with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The order of matrix multiplication by weight matrices is given by the arrangement of weights into columns and rows, see below and in other functions.\n",
    "    W1 = np.random.rand(l1_size, input_size) - 0.5   # Each column is applied to respective input value, weights for each neuron of the first hidden layer are in rows.\n",
    "    W2 = np.random.rand(l2_size, l1_size) - 0.5      # Each column is applied to respective neuron of the first hidden layer, weights for each neuron of the second hidden layer are in rows.\n",
    "    W3 = np.random.rand(output_size, l2_size) - 0.5  # Each column is applied to respective neuron of the second hidden layer, weights for each neuron of the out layer are in rows.\n",
    "    return W1, W2, W3\n",
    "\n",
    "def weights_init(sizes: list) -> list:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :sizes: List of layer sizes, first value specifies number of input values, last value specifies number of output neurons. \n",
    "            Number of initilizes weight matrices is one less than lenght of the given list.\n",
    "    :return: List of randomly initilized weight matrices with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    weights = [] \n",
    "    for i in range(len(sizes) - 1):\n",
    "        # The order of matrix multiplication by weight matrices is given by the arrangement of weights into columns and rows, see below and in other functions.\n",
    "        weights.append(np.random.rand(sizes[i + 1], sizes[i]) - 0.5) # Columns represent weights for Nth layer values, rows represent weights for N+1th layer values.\n",
    "    return weights\n",
    "\n",
    "def biases_init_2L(size: int, output_size = 10) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly initilizes biases with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :size: Number of neurons in a hidden layer.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of randomly initilized bias vectors with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The biases are column vectors\n",
    "    B1 = np.random.rand(size, 1) - 0.5          # One bias for each neuron in the hidden layer.\n",
    "    B2 = np.random.rand(output_size, 1) - 0.5   # One bias for each neuron in the output layer.\n",
    "    return B1, B2\n",
    "\n",
    "def biases_init_3L(l1_size, l2_size, output_size = CLASSES_COUNT):\n",
    "    \"\"\"\n",
    "    Randomly initilizes biases with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :l1_size: Number of neurons in a first hidden layer.\n",
    "    :l2_size: Number of neurons in a second hidden layer.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of randomly initilized bias vectors with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The biases are column vectors\n",
    "    B1 = np.random.rand(l1_size, 1) - 0.5       # One bias for each neuron in the first hidden layer.\n",
    "    B2 = np.random.rand(l2_size, 1) - 0.5       # One bias for each neuron in the second hidden layer.\n",
    "    B3 = np.random.rand(output_size, 1) - 0.5   # One bias for each neuron in the output layer.\n",
    "    return B1, B2, B3\n",
    "\n",
    "def biases_init(sizes):\n",
    "    biases = []\n",
    "    for i in range(1, len(sizes)): # skips the input valies size\n",
    "        biases.append(np.random.rand(sizes[i], 1) - 0.5)    # Column vectors for each layer.\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabb76d",
   "metadata": {},
   "source": [
    "### Adjusting weights and biases after a training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6416852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_adjust_2L(W1: npt.NDArray, W2: npt.NDArray, dW1: npt.NDArray, dW2: npt.NDArray, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :dW1: First layer gradient matrix.\n",
    "    :dW2: Second layer gradient matrix.\n",
    "    :return: Tuple of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    return W1, W2\n",
    "\n",
    "def weights_adjust_3L(W1: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, dW1: npt.NDArray, dW2: npt.NDArray, dW3: npt.NDArray, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :W2: Third layer weight matrix.\n",
    "    :dW1: First layer gradient matrix.\n",
    "    :dW2: Second layer gradient matrix.\n",
    "    :dW2: Third layer gradient matrix.\n",
    "    :return: Tuple of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    return W1, W2, W3\n",
    "\n",
    "def weights_adjust(weights: list, weight_gradients: list, learning_rate: float) -> list:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :weights: List of weight matrices.\n",
    "    :weight_gradients: List of gradient matrices of the same shape as respective weight matrices.\n",
    "    :return: List of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - learning_rate * weight_gradients[i]\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def biases_adjust_2L(B1: npt.ArrayLike, B2: npt.ArrayLike, dB1: float, dB2: float, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the gradients and learning rate.\n",
    "\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :dB1: First layer bias difference (scalar value).\n",
    "    :dB2: Second layer bias difference (scalar value).\n",
    "    :return: Tuple of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "    return B1, B2\n",
    "\n",
    "def biases_adjust_3L(B1: npt.ArrayLike, B2: npt.ArrayLike, B3: npt.ArrayLike, dB1: float, dB2: float, dB3: float, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the difference and learning rate.\n",
    "\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :B2: Third layer bias column vector.\n",
    "    :dB1: First layer bias difference (scalar value).\n",
    "    :dB2: Second layer bias difference (scalar value).\n",
    "    :dB2: Third layer bias difference (scalar value).\n",
    "    :return: Tuple of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "    B3 = B3 - learning_rate * dB3\n",
    "    return B1, B2, B3\n",
    "\n",
    "def biases_adjust(biases: npt.ArrayLike, bias_diff: npt.ArrayLike, learning_rate: float) -> list:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the gradients and learning rate.\n",
    "\n",
    "    :biases: List of bias column vectors.\n",
    "    :bias_diff: List of bias differences (scalar values).\n",
    "    :return: List of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    for i in range(len(biases)):\n",
    "        biases[i] = biases[i] - learning_rate * bias_diff[i]\n",
    "    \n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b90ee7",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6dbe149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_2L(W1: npt.NDArray, W2: npt.NDArray, B1: npt.ArrayLike, B2: npt.ArrayLike, training_data: npt.ArrayLike) -> tuple:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: Tuple of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    Z1 = W1.dot(training_data) + B1 # Matrix multiplication of the training samples and first layer weights with addition of biases. The calculated matrix shape is: rows = W1.rows, cols = training_data.cols.\n",
    "    L1 = ReLU(Z1)                   # Nonlinear activation function applied on the first layer values. The shape of the resulting matrix is the stays the same.\n",
    "    Z2 = W2.dot(L1) + B2            # Matrix multiplication of the first layer values and second layer weights with addition of biases. The calculated matrix shape is: rows = W2.rows, cols = training_data.cols.\n",
    "    L2 = softmax(Z2)                # Transformation of the second layer outputs to a probability distribution. The shape of the resulting matrix is the stays the same.\n",
    "    return L1, L2\n",
    "\n",
    "def forward_prop_3L(W1: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, B1: npt.ArrayLike, B2: npt.ArrayLike, B3: npt.ArrayLike, training_data: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :W3: Third layer weight matrix.\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :B3: Third layer bias column vector.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: Tuple of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    Z1 = W1.dot(training_data) + B1 # Matrix multiplication of the training samples and first layer weights with addition of biases. The calculated matrix shape is: rows = W1.rows, cols = training_data.cols.\n",
    "    L1 = ReLU(Z1)                   # Nonlinear activation function applied on the first layer values. The shape of the resulting matrix is the stays the same.\n",
    "    Z2 = W2.dot(L1) + B2            # Matrix multiplication of the first layer values and second layer weights with addition of biases. The calculated matrix shape is: rows = W2.rows, cols = training_data.cols.\n",
    "    L2 = ReLU(Z2)                   # Nonlinear activation function applied on the second layer values. The shape of the resulting matrix is the stays the same.\n",
    "    Z3 = W3.dot(L2) + B3            # Matrix multiplication of the second layer values and third layer weights with addition of biases. The calculated matrix shape is: rows = W3.rows, cols = training_data.cols.\n",
    "    L3 = softmax(Z3)                # Transformation of the third layer outputs to a probability distribution. The shape of the resulting matrix is the stays the same.\n",
    "    return L1, L2, L3\n",
    "\n",
    "def forward_prop(weights: npt.NDArray, biases: npt.ArrayLike, activations: list, training_data: npt.NDArray) -> list:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :weights: List of weight matrices.\n",
    "    :biases: List of bias column vectors.\n",
    "    :activations: List of nonlinear activation functions.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: List of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    input = training_data\n",
    "    for i in range(len(weights)):\n",
    "        layers.append(activations[i](weights[i].dot(input) + biases[i])) # Matrix multiplication of the Nth layer and weights of the N+1th layer with addition of biases and aplication of nonlinear activation function.\n",
    "        input = layers[i]   # Save the current layer\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fdef0",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a41a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_2L(L1: npt.NDArray, L2: npt.NDArray, W2: npt.NDArray, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :L1: Matrix of first layer values.\n",
    "    :L2: Matrix of second layer values.\n",
    "    :W2: Matrix of second layer weights.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of matrices with weight and bias gradients for each layer.\n",
    "    \"\"\"\n",
    "    m = one_hot_labels.shape[1] # The number of training samples. \n",
    "    dZ2 = L2 - one_hot_labels   # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    dW2 = dZ2.dot(L1.T) / m     # Matrix multiplication of the output layer errors and hidden layer values normalized by the number of training samples. The calculated matrix shape is: rows = one_hot_labels.rows == W2.rows (10 in case of digit classification), cols = L1.rows == W2.cols (L1 is transposed).\n",
    "    dB2 = np.sum(dZ2) / m       # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(L1) # Back propagating the errors from output layer to the hidden layer by matrix multiplication of the transponed weight matrix and errors multiplied by the ReLU derivation. The calculated matrix shape is: rows: W2.cols == W1.rows (W2 is transposed), cols = one_hot_labels.cols (number of training samples).\n",
    "    dW1 = dZ1.dot(training_data.T) / m   # Matrix multiplication of the hidden layer errors and input values normalized by the number of training samples. The calculated matrix shape is: rows = dZ1.rows == W1.rows, cols = training_data.rows == W1.cols (training_data is transposed, in case of images 28x28 it is 784).\n",
    "    dB1 = np.sum(dZ1) / m                # Arithmetic average of the hidden layer errors. The result is a scalar value.\n",
    "    return dW1, dW2, dB1, dB2\n",
    "\n",
    "def back_prop_3L(L1: npt.NDArray, L2: npt.NDArray, L3: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :L1: Matrix of first layer values.\n",
    "    :L2: Matrix of second layer values.\n",
    "    :L3: Matrix of third layer values.\n",
    "    :W2: Matrix of second layer weights.\n",
    "    :W3: Matrix of third layer weights.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of matrices with weight and bias gradients for each layer.\n",
    "    \"\"\"\n",
    "    m = one_hot_labels.shape[1]  # The number of training samples.\n",
    "    dZ3 = L3 - one_hot_labels    # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    dW3 = dZ3.dot(L2.T) / m      # Matrix multiplication of the output layer errors and hidden layer values normalized by the number of training samples. The calculated matrix shape is: rows = one_hot_labels.rows == W3.rows (10 in case of digit classification), cols = L2.rows == W3.cols (L2 is transposed).\n",
    "    dB3 = np.sum(dZ3) / m        # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(L2) # Back propagating the errors from output layer to the last hidden layer by matrix multiplication of the transponed weight matrix and errors by the ReLU derivation. The calculated matrix shape is: rows: W3.cols == W2.rows (W3 is transposed), cols = one_hot_labels.cols (number of training samples).\n",
    "    dW2 = dZ2.dot(L1.T) / m      # Matrix multiplication of the last hidden layer errors and first layer values normalized by the number of training samples. The calculated matrix shape is: rows = dZ2.rows == W2.rows, cols = L1.rows == W2.cols (L1 is transposed).\n",
    "    dB2 = np.sum(dZ2) / m        # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(L1) # Back propagating the errors from last hidden layer to the first (second to last) hidden layer by matrix multiplication of the transponed weight matrix and errors by the ReLU derivation. The calculated matrix shape is: rows: W2.cols == W1.rows (W2 is transposed), cols = dZ2.cols (number of training samples).\n",
    "    dW1 = dZ1.dot(training_data.T) / m   # Matrix multiplication of the first hidden layer errors and input values normalized by the number of training samples. The calculated matrix shape is: rows = dZ1.rows == W1.rows, cols = training_data.rows == W1.cols (training_data is transposed, in case of images 28x28 it is 784).\n",
    "    dB1 = np.sum(dZ1) / m                # Arithmetic average of the errors. The result is a scalar value.\n",
    "    return dW1, dW2, dW3, dB1, dB2, dB3\n",
    "\n",
    "def back_prop(layers: list, weights: list, activation_derivs: list, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :layers: List of layer matrices.\n",
    "    :weights: List of weight matrices.\n",
    "    :activation_derivs: List of derivations of nonlinear activation functions.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of lists of matrices with weight and bias differences for each layer.\n",
    "    \"\"\"\n",
    "    layers_count = len(layers)                  # The number of layers.\n",
    "    m = one_hot_labels.shape[1]                 # The number of training samples.\n",
    "    weight_gradients = [None] * layers_count    # Initialization of weight gradients list\n",
    "    bias_diff = [None] * layers_count           # Initialization of bias differences list\n",
    "\n",
    "    current_deriv = layers[layers_count - 1] - one_hot_labels          # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    for i in range(layers_count - 1, 0, -1):  # Ranging from N to 1\n",
    "        weight_gradients[i] = current_deriv.dot(layers[i - 1].T) / m   # Matrix multiplication of the N+1th hidden layer errors and Nth hidden layer values normalized by the number of training samples. The shape of the resulting matrix changes with iterations, see above.\n",
    "        bias_diff[i] = np.sum(current_deriv) / m                       # Arithmetic average of the errors - scalar value.\n",
    "        current_deriv = weights[i].T.dot(current_deriv) * activation_derivs[i - 1](layers[i - 1]) # Back propagating the errors from N+1th layer to the Nth layer by matrix multiplication of the transponed weight matrix and errors by the derivation of activation function. The shape of the resulting matrix changes with iterations, see above.\n",
    "    \n",
    "    # Calculation for the first layer\n",
    "    weight_gradients[0] = current_deriv.dot(training_data.T) / m    # Matrix multiplication of the first hidden layer errors and input values normalized by the number of training samples.\n",
    "    bias_diff[0] = np.sum(current_deriv) / m                        # Arithmetic average of the errors.\n",
    "\n",
    "    return weight_gradients, bias_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced063f",
   "metadata": {},
   "source": [
    "### Gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e515b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend_2L(training_data: npt.NDArray, labels: npt.ArrayLike, L1_size: int, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network with 1 hidden layer using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    W1, W2 = weights_init_2L(L1_size)\n",
    "    B1, B2 = biases_init_2L(L1_size)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training\n",
    "    for _ in range(iterations):\n",
    "        L1, L2 = forward_prop_2L(W1, W2, B1, B2, training_data)\n",
    "        dW1, dW2, dB1, dB2 = back_prop_2L(L1, L2, W2, training_data, labels)\n",
    "        W1, W2 = weights_adjust_2L(W1, W2, dW1, dW2, learning_rate)\n",
    "        B1, B2 = biases_adjust_2L(B1, B2, dB1, dB2, learning_rate)\n",
    "\n",
    "    return W1, W2, B1, B2\n",
    "\n",
    "def gradient_descend_3L(training_data: npt.NDArray, labels: npt.ArrayLike, L1_size: int, L2_size: int, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network with 2 hidden layers using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :L2_size: Number of neurons in the second hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    W1, W2, W3 = weights_init_3L(L1_size, L2_size)\n",
    "    B1, B2, B3 = biases_init_3L(L1_size, L2_size)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training \n",
    "    for _ in range(iterations):\n",
    "        L1, L2, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, training_data)\n",
    "        dW1, dW2, dW3, dB1, dB2, dB3 = back_prop_3L(L1, L2, L3, W2, W3, training_data, labels)\n",
    "        W1, W2, W3 = weights_adjust_3L(W1, W2, W3, dW1, dW2, dW3, learning_rate)\n",
    "        B1, B2, B3 = biases_adjust_3L(B1, B2, B3, dB1, dB2, dB3, learning_rate)\n",
    "\n",
    "    return W1, W2, W3, B1, B2, B3\n",
    "\n",
    "def gradient_descend(training_data: npt.NDArray, labels: npt.ArrayLike, weight_sizes: list, activations: list, activation_derivs: list, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network given number of layers using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :weigh_sizes: List of numbers of neurons in each layer including the input layer.\n",
    "    :activations: List of activation functions for each hidden layer and output layer.\n",
    "    :activation_derivs: List of derivation of activation functions each hidden layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of list of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    weights = weights_init(weight_sizes)\n",
    "    biases = biases_init(weight_sizes)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training\n",
    "    for _ in range(iterations):\n",
    "        layers = forward_prop(weights, biases, activations, training_data)\n",
    "        weight_gradients, bias_diff = back_prop(layers, weights, activation_derivs, training_data, labels)\n",
    "        weights = weights_adjust(weights, weight_gradients, learning_rate)\n",
    "        biases = biases_adjust(biases, bias_diff, learning_rate)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6c230",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97ebb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2L(training_data: npt.NDArray, training_labels: npt.ArrayLike, L1_size = L1_SIZE, iterations = ITERATIONS, learning_rate = LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains a neural network with one hidden layer using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :training_labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    W1, W2, B1, B2 = gradient_descend_2L(training_data, training_labels, L1_size, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    np.save(\"W1_2L.npy\", W1)\n",
    "    np.save(\"W2_2L.npy\", W2)\n",
    "    np.save(\"B1_2L.npy\", B1)\n",
    "    np.save(\"B2_2L.npy\", B2)\n",
    "\n",
    "def train_3L(training_data, training_labels, L1_size = L1_SIZE, L2_size = L2_SIZE, iterations = ITERATIONS, learning_rate = LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains a neural network with two hidden layers using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :training_labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :L2_size: Number of neurons in the second hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    W1, W2, W3, B1, B2, B3 = gradient_descend_3L(training_data, training_labels, L1_size, L2_size, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    np.save(\"W1_3L.npy\", W1)\n",
    "    np.save(\"W2_3L.npy\", W2)\n",
    "    np.save(\"W3_3L.npy\", W3)\n",
    "    np.save(\"B1_3L.npy\", B1)\n",
    "    np.save(\"B2_3L.npy\", B2)\n",
    "    np.save(\"B3_3L.npy\", B3)\n",
    "\n",
    "def train(training_data: npt.NDArray, training_labels: npt.ArrayLike, weight_sizes: list, activations: list, activation_derivs: list, iterations = ITERATIONS, learning_rate = LEARNING_RATE, \n",
    "          files_id = dt.now().strftime(\"%d-%m-%Y_%H:%M:%S\")):\n",
    "    \"\"\"\n",
    "    Trains a neural network with a given number of hidden layers using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :weigh_sizes: List of numbers of neurons in each layer including the input layer.\n",
    "    :activations: List of activation functions for each hidden layer and output layer.\n",
    "    :activation_derivs: List of derivation of activation functions each hidden layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of list of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    weights, biases = gradient_descend(training_data, training_labels, weight_sizes, activations, activation_derivs, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    for i in range(len(weights)):\n",
    "        j = str(i + 1)\n",
    "        np.save(\"W\" + j + \"_\" + files_id + \".npy\", weights[i])\n",
    "        np.save(\"B\" + j + \"_\" + files_id + \".npy\", biases[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef8553",
   "metadata": {},
   "source": [
    "### Funtions for loading of training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f6b3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\")\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    sample_count = training_data.shape[0]\n",
    "    training_data = np.reshape(training_data, (sample_count, -1)).T / 255\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\")\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    sample_count = test_data.shape[0]\n",
    "    test_data = np.reshape(test_data, (sample_count, -1)).T / 255\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4aab6",
   "metadata": {},
   "source": [
    "### Functions for result assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "432b605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_results_2L():\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    # Loading of the trained weights and biases.\n",
    "    W1 = np.load(\"W1_2L.npy\")\n",
    "    W2 = np.load(\"W2_2L.npy\")\n",
    "    B1 = np.load(\"B1_2L.npy\")\n",
    "    B2 = np.load(\"B2_2L.npy\")\n",
    "    print(\"\\n###################### 2 layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    _, L2 = forward_prop_2L(W1, W2, B1, B2, training_data)                 # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(L2, training_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    _, L2 = forward_prop_2L(W1, W2, B1, B2, test_data)             # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(L2, test_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(L2, test_labels, test_data)\n",
    "\n",
    "def assess_results_3L():\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with two hidden layer.\n",
    "    \"\"\"\n",
    "    # Loading of the trained weights and biases.\n",
    "    W1 = np.load(\"W1_3L.npy\")\n",
    "    W2 = np.load(\"W2_3L.npy\")\n",
    "    W3 = np.load(\"W3_3L.npy\")\n",
    "    B1 = np.load(\"B1_3L.npy\")\n",
    "    B2 = np.load(\"B2_3L.npy\")\n",
    "    B3 = np.load(\"B3_3L.npy\")\n",
    "    print(\"\\n###################### 3 layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    _, _, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, training_data)      # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(L3, training_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    _, _, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, test_data)   # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(L3, test_labels))  # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(L3, test_labels, test_data)\n",
    "\n",
    "def assess_results(activations: list, files_id = \"\"):\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with given number of layers including the output layer.\n",
    "\n",
    "    :activations: Activation functions for each hidden layer and for the output layer.\n",
    "    :files_id: Id in the file names of the weights and biases.\n",
    "    \"\"\"\n",
    "    layers_count = len(activations)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    # Loading of the trained weights and biases.\n",
    "    for i in range(layers_count):\n",
    "        j = str(i + 1)\n",
    "        weights.append(np.load(\"W\" + j + \"_\" + files_id + \".npy\"))\n",
    "        biases.append(np.load(\"B\" + j + \"_\" + files_id + \".npy\"))\n",
    "    \n",
    "    print(\"\\n###################### \" + str(layers_count) + \" layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    last_layer = forward_prop(weights, biases, activations, training_data)[layers_count - 1] # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(last_layer, training_labels))           # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    last_layer = forward_prop(weights, biases, activations, test_data)[layers_count - 1]     # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(last_layer, test_labels))                   # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(last_layer, test_labels, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f136be3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66870650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dB1  1.789861814668362 (60000,)\n",
      "dB1  0.7747461507313812 (60000,)\n",
      "dB1  0.5822687881156722 (60000,)\n",
      "dB1  0.34759051712157424 (60000,)\n",
      "dB1  0.2910259269995468 (60000,)\n",
      "dB1  0.2181509372789305 (60000,)\n",
      "dB1  0.17940231817778304 (60000,)\n",
      "dB1  0.14057825353337874 (60000,)\n",
      "dB1  0.11501808553553791 (60000,)\n",
      "dB1  0.09303067971053905 (60000,)\n",
      "dB1  0.0759513602197056 (60000,)\n",
      "dB1  0.060137926127678605 (60000,)\n",
      "dB1  0.04814020460852757 (60000,)\n",
      "dB1  0.03719162472792257 (60000,)\n",
      "dB1  0.028239929287058037 (60000,)\n",
      "dB1  0.021397758088115695 (60000,)\n",
      "dB1  0.015592130213674667 (60000,)\n",
      "dB1  0.010625899911704312 (60000,)\n",
      "dB1  0.006582425202308468 (60000,)\n",
      "dB1  0.002669330028510969 (60000,)\n",
      "dB1  -0.0007004206123804344 (60000,)\n",
      "dB1  -0.0034943080270543295 (60000,)\n",
      "dB1  -0.0058497706742344485 (60000,)\n",
      "dB1  -0.007816654012766176 (60000,)\n",
      "dB1  -0.0093308525144009 (60000,)\n",
      "dB1  -0.010324456708920488 (60000,)\n",
      "dB1  -0.011866930868549347 (60000,)\n",
      "dB1  -0.012593715341314817 (60000,)\n",
      "dB1  -0.013439079954007343 (60000,)\n",
      "dB1  -0.014148374648745982 (60000,)\n",
      "dB1  -0.014817868974929031 (60000,)\n",
      "dB1  -0.015247405812354259 (60000,)\n",
      "dB1  -0.015601769946616835 (60000,)\n",
      "dB1  -0.016149739179853406 (60000,)\n",
      "dB1  -0.01644009997655148 (60000,)\n",
      "dB1  -0.016614724876703905 (60000,)\n",
      "dB1  -0.016529357061409934 (60000,)\n",
      "dB1  -0.016237317468842917 (60000,)\n",
      "dB1  -0.016549592417894103 (60000,)\n",
      "dB1  -0.01663053314000737 (60000,)\n",
      "dB1  -0.016825684132024614 (60000,)\n",
      "dB1  -0.016818103893704434 (60000,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_NN.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_data, training_labels \u001b[39m=\u001b[39m load_training_data()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_2L(training_data, training_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_3L(training_data, training_labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(training_data, training_labels, [PIXELS_PER_IMAGE, \u001b[39m112\u001b[39m, L1_SIZE, L2_SIZE, CLASSES_COUNT], [ReLU, ReLU, ReLU, softmax], \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       [ReLU_deriv, ReLU_deriv, ReLU_deriv], files_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m4L\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_NN.ipynb Cell 24\u001b[0m in \u001b[0;36mtrain_2L\u001b[0;34m(training_data, training_labels, L1_size, iterations, learning_rate)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mTrains a neural network with one hidden layer using gradient descend and saves the result.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m:learning_rate: Learning rate used to adjust weights and biases.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m W1, W2, B1, B2 \u001b[39m=\u001b[39m gradient_descend_2L(training_data, training_labels, L1_size, iterations, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# saving results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mW1_2L.npy\u001b[39m\u001b[39m\"\u001b[39m, W1)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_NN.ipynb Cell 24\u001b[0m in \u001b[0;36mgradient_descend_2L\u001b[0;34m(training_data, labels, L1_size, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     L1, L2 \u001b[39m=\u001b[39m forward_prop_2L(W1, W2, B1, B2, training_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     dW1, dW2, dB1, dB2 \u001b[39m=\u001b[39m back_prop_2L(L1, L2, W2, training_data, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     W1, W2 \u001b[39m=\u001b[39m weights_adjust_2L(W1, W2, dW1, dW2, learning_rate)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_NN.ipynb Cell 24\u001b[0m in \u001b[0;36mforward_prop_2L\u001b[0;34m(W1, W2, B1, B2, training_data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop_2L\u001b[39m(W1: npt\u001b[39m.\u001b[39mNDArray, W2: npt\u001b[39m.\u001b[39mNDArray, B1: npt\u001b[39m.\u001b[39mArrayLike, B2: npt\u001b[39m.\u001b[39mArrayLike, training_data: npt\u001b[39m.\u001b[39mArrayLike) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Classifies all training samples in to classes according to the given weights and biases.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    :return: Tuple of matrices with values for each layer.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     Z1 \u001b[39m=\u001b[39m W1\u001b[39m.\u001b[39;49mdot(training_data) \u001b[39m+\u001b[39m B1 \u001b[39m# Matrix multiplication of the training samples and first layer weights with addition of biases. The calculated matrix shape is: rows = W1.rows, cols = training_data.cols.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     L1 \u001b[39m=\u001b[39m ReLU(Z1)                   \u001b[39m# Nonlinear activation function applied on the first layer values. The shape of the resulting matrix is the stays the same.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_NN.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     Z2 \u001b[39m=\u001b[39m W2\u001b[39m.\u001b[39mdot(L1) \u001b[39m+\u001b[39m B2            \u001b[39m# Matrix multiplication of the first layer values and second layer weights with addition of biases. The calculated matrix shape is: rows = W2.rows, cols = training_data.cols.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "train_2L(training_data, training_labels)\n",
    "train_3L(training_data, training_labels)\n",
    "train(training_data, training_labels, [PIXELS_PER_IMAGE, 112, L1_SIZE, L2_SIZE, CLASSES_COUNT], [ReLU, ReLU, ReLU, softmax], \n",
    "      [ReLU_deriv, ReLU_deriv, ReLU_deriv], files_id=\"4L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c77a9",
   "metadata": {},
   "source": [
    "### Result assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_results_2L()\n",
    "assess_results_3L()\n",
    "assess_results([ReLU, ReLU, ReLU, softmax], \"4L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01af5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO dropout\n",
    "pKeep = 0.8\n",
    "weights = np.ones([5, 5])\n",
    "binary_value = np.random.rand(weights.shape[0], weights.shape[1]) < pKeep\n",
    "res = np.multiply(weights, binary_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
