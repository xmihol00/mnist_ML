{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2f6df1",
   "metadata": {},
   "source": [
    "# Basic Neural Network\n",
    "This python notebok contains code for very basic neural network with explanations of the underlining concepts. The network is design to recognize handwritten digits from the MNIST dataset. Each digit is a 28x28 monochrome picture.\n",
    "\n",
    "It is a fully connected neural network programed mainly using numpy and its matrix operations. The code is not written convntionally as you would encounter in popular neural network/machine learning libraries, but hopefuly easier to understand. Slight modifications can be made, so that the network could be used for any reasonable dataset, although the results might not be amazing.\n",
    "\n",
    "### Letarature\n",
    "https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "\n",
    "https://video1.fit.vutbr.cz/av/records-categ.php?id=1840\n",
    "\n",
    "https://hmkcode.com/ai/backpropagation-step-by-step/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6aa62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "\n",
    "L1_SIZE = 56\n",
    "L2_SIZE = 28\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f253a",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc689fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth labels.\n",
    "    :return: The accuracy as a real number between 0 and 1. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aacbd",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "These non-linear functions are used on outputs of each layer. Non-linearity is necessary, otherwise creating more layers does not yield better results, the same results could be calculated using just single layer with sufficient number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d68cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, L)\n",
    "\n",
    "def ReLU_deriv(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return L > 0\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indices with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def sigmoid_deriv(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indices with value x returns (1 / (1 + e^(-x))) * (1 - (1 / (1 + e^(-x)))).\n",
    "    \"\"\"\n",
    "    sigm = sigmoid(L)\n",
    "    return sigm * (1 - sigm)\n",
    "\n",
    "def softmax(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a column of N values (in case of digits 10 values) to a probability distribution of N outcomes for each column of a matrix.\n",
    "    Columns shoud be the outputs of a neural network.\n",
    "\n",
    "    :L: Values of an output layer ordered into columns for each sample.\n",
    "    :return: For all indices of the given matrix returns the probability of a given index in its column.\n",
    "    \"\"\"\n",
    "    exp = np.exp(L)\n",
    "    return exp / np.sum(exp, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb692aa",
   "metadata": {},
   "source": [
    "### Initialization of Weights And Biases, Explanation of Their Arrangement\n",
    "Correct initialization of weights and biases is important as it can have severe impact on the performance of a neural network. Biases are usually initialized with zeros, as is also done here, weights are usually initialized randomly but in a more sofisticated way than how it is done below.\n",
    "\n",
    "The calculation of output values (values of neurons) of a given layer is done in a following way. Given $I$ (let's say 20) input values and $O$ (let's say 10) output values, there is one weight for each input value per each output value (each input value has 10 weights, each output value has 20 weights, if inputs and outputs are nodes and weigths are edges, it is a complete bipartite graph), that is $W_{size} = I_{size} \\cdot O_{size}$ (there are 200 weights). Biases are simplier, there is just one bias per output value (that is 10). First output value is calculated as the sum of all input values multiplied respectively by the first 20 weights and a bias: \n",
    "$$\n",
    "o0 = w0 \\cdot i0 + w1 \\cdot i1 + w2 \\cdot i2 + ... + w19 \\cdot i19 + b0\n",
    "$$\n",
    "Similarly this must be done for the remaining 9 output values.\n",
    "\n",
    "The calculation of all output values can be done simultaneously by a matrix multiplication (by the weights) and addition (of the biases), if the weights, biases and input values are arranged correctly. Input values must be arranged in a column vector (for the example that is 20 values) as well as the biases (10 values). Weights are arranged to a matrix, where the number of columns is the number of input values (that is 20 columns) and the number of rows is the number of output values of the given layer (that is 10). The matrix multiplication must be done in a correct order, so that the dimensions are correctly aligned, that is: \n",
    "$$\n",
    "    O = \n",
    "    W \\times I + B \n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "        w_{0} & w_{2} & \\cdots & w_{19}\\\\\n",
    "        w_{20} & w_{21} & \\cdots & w_{39}\\\\ \n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "        w_{80} & w_{81} & \\cdots & w_{99} \n",
    "    \\end{bmatrix}\n",
    "    \\times\n",
    "    \\begin{bmatrix}\n",
    "        i_{0}\\\\\n",
    "        i_{1}\\\\ \n",
    "        \\vdots\\\\ \n",
    "        i_{19}\n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "        b_{0}\\\\\n",
    "        b_{1}\\\\ \n",
    "        \\vdots\\\\ \n",
    "        b_{9}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        o_{0}\\\\\n",
    "        o_{1}\\\\ \n",
    "        \\vdots\\\\ \n",
    "        o_{9}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "The output values are then arranged in a column vector (with 10 values), which is convinient, as this vector can be immidiately used as an input vector for next layer. The classification, which a neural network does, is done by chaining these matrix multiplications of each layer.\n",
    "\n",
    "Note, that this input/output column vector holds values just for a single training sample (let's say the input values are values of pixels of a 4 by 5 image). Calculation of outputs for all (or any sufficient number - a batch) training samples simultaneously, as it is needed to perform the learning of a neural network, can be done by stacking more column vectors with values of training samples together, i.e. creating a matrix where each columns represent values for each training sample (let's say 1000 training samples, then the matrix has 20 rows - values of pixels; and 1000 columns - each for one training sample). Also note, that the indexing of scalar values inside matrices changed to a more consistent notation, that is indexing by rows and columns. It is good to notice the relation of $i$, $j$ and $k$ indices.\n",
    "$$\n",
    "    O = \n",
    "    W \\times I + B \n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "        w_{00} & w_{01} & \\cdots & w_{0j}\\\\\n",
    "        w_{10} & w_{11} & \\cdots & w_{0j}\\\\ \n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "        w_{i0} & w_{i1} & \\cdots & w_{ij} \n",
    "    \\end{bmatrix}\n",
    "    \\times\n",
    "    \\begin{bmatrix}\n",
    "        i_{00} & i_{01} & \\cdots & i_{0k}\\\\\n",
    "        i_{10} & i_{11} & \\cdots & i_{1k}\\\\ \n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "        i_{j0} & i_{j1} & \\cdots & i_{jk} \n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "        b_{0}\\\\\n",
    "        b_{1}\\\\ \n",
    "        \\vdots\\\\ \n",
    "        b_{i}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        o_{00} & o_{01} & \\cdots & o_{0k}\\\\\n",
    "        o_{10} & o_{11} & \\cdots & o_{1k}\\\\ \n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "        o_{i0} & o_{i1} & \\cdots & o_{ik} \n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "565c9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_2L(size: int, input_size = PIXELS_PER_IMAGE, output_size = CLASSES_COUNT) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :size: Number of neurons in a hidden layer.\n",
    "    :input_size: Number of input values.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of randomly initilized weight matrices with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The number of rows of a weight matrix is alligned with the number of columns of the following matrix, as descibed above.\n",
    "    W1 = np.random.rand(size, input_size) - 0.5  # matrix of 'size' rows and 'input_size' columns\n",
    "    W2 = np.random.rand(output_size, size) - 0.5 # matrix of 'output_size' (number of classes, in case of digits - 10) rows and 'size' columns\n",
    "    return W1, W2\n",
    "\n",
    "def weights_init_3L(l1_size: int, l2_size: int, input_size = PIXELS_PER_IMAGE, output_size = CLASSES_COUNT) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :l1_size: Number of neurons in a first hidden layer.\n",
    "    :l2_size: Number of neurons in a second hidden layer.\n",
    "    :input_size: Number of input values.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of randomly initilized weight matrices with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    # The number of rows of a weight matrix is alligned with the number of columns of the following matrix, as descibed above.\n",
    "    W1 = np.random.rand(l1_size, input_size) - 0.5   # matrix of 'l1_size' rows and 'input_size' columns\n",
    "    W2 = np.random.rand(l2_size, l1_size) - 0.5      # matrix of 'l2_size' rows and 'l1_size' columns\n",
    "    W3 = np.random.rand(output_size, l2_size) - 0.5  # matrix of 'output_size' (number of classes, in case of digits - 10) rows and 'l2_size' columns\n",
    "    return W1, W2, W3\n",
    "\n",
    "def weights_init(sizes: list) -> list:\n",
    "    \"\"\"\n",
    "    Randomly initilizes weights with real numbers between -0.5 and 0.5.\n",
    "\n",
    "    :sizes: List of layer sizes, first value specifies number of input values, last value specifies number of output neurons. \n",
    "            The number of initilizes weight matrices is one less than lenght of the given list.\n",
    "    :return: List of randomly initilized weight matrices with real numbers between -0.5 and 0.5.\n",
    "    \"\"\"\n",
    "    weights = [] \n",
    "    for i in range(len(sizes) - 1):\n",
    "        # The number of rows of a weight matrix is alligned with the number of columns of the following matrix, as descibed above.\n",
    "        weights.append(np.random.rand(sizes[i + 1], sizes[i]) - 0.5)\n",
    "    return weights\n",
    "\n",
    "def biases_init_2L(size: int, output_size = 10) -> tuple:\n",
    "    \"\"\"\n",
    "    Initilizes two bias column vectors with zeros.\n",
    "\n",
    "    :size: Number of neurons in a hidden layer.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of zero initilized bias vectors.\n",
    "    \"\"\"\n",
    "    # Biases are column vectors, in numpy those are matrices with given number of rows and only one column.\n",
    "    B1 = np.zeros((size, 1))          # matrix of 'size' rows and only one column\n",
    "    B2 = np.zeros((output_size, 1))   # matrix of 'output_size' rows and only one column\n",
    "    return B1, B2\n",
    "\n",
    "def biases_init_3L(l1_size, l2_size, output_size = CLASSES_COUNT):\n",
    "    \"\"\"\n",
    "    Initilizes three bias column vectors with zeros.\n",
    "\n",
    "    :l1_size: Number of neurons in a first hidden layer.\n",
    "    :l2_size: Number of neurons in a second hidden layer.\n",
    "    :output_size: Number of neurons in an output layer.\n",
    "    :return: Tuple of zero initilized bias vectors.\n",
    "    \"\"\"\n",
    "    # Biases are column vectors, in numpy those are matrices with given number of rows and only one column.\n",
    "    B1 = np.zeros((l1_size, 1))      # One bias for each neuron in the first hidden layer.\n",
    "    B2 = np.zeros((l2_size, 1))      # One bias for each neuron in the second hidden layer.\n",
    "    B3 = np.zeros((output_size, 1))  # One bias for each neuron in the output layer.\n",
    "    return B1, B2, B3\n",
    "\n",
    "def biases_init(sizes: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Initilizes bias column vectors with zeros.\n",
    "\n",
    "    :size: List of layer sizes, first value specifies number of input values, last value specifies number of output neurons. \n",
    "           The number of initilizes weight matrices is one less than lenght of the given list.\n",
    "    :return: List of zero initilized bias vectors.\n",
    "    \"\"\"\n",
    "    biases = []\n",
    "    # Biases are column vectors, in numpy those are matrices with given number of rows and only one column.\n",
    "    for i in range(1, len(sizes)): # starting from 1, skipping the input value size\n",
    "        biases.append(np.zeros((sizes[i], 1)))  \n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b90ee7",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "Forward propagation is an algorithm, which calculates the output of a neural network for given input values. There are three operations successively performed for each layer, those are:\n",
    "1. the matrix of input values or the output from previous layer is multiplied by the weight matrix of the respective layer,\n",
    "2. biases of the respective layer are added to the result of matrix multiplication,\n",
    "3. an activation functions is applied to the result.\n",
    "\n",
    "The output values of a neural network with two hidden layer using ReLU as an activation function and an output layer with Softmax activation function would look like:\n",
    "$$\n",
    "O = softmax(W_3 \\times ReLU(W_2 \\times ReLU(W_1 \\times I + B_1) + B_2) + B_3)\n",
    "$$\n",
    "\n",
    "Note that, on a trained neural network the resuts of any hidden layer can be discarder, after they were used for calculating the results of a next hidden layer, as only the results of the output layer are needed. But to train a neural network, values of all hiden layers must be stored. These values are then used to improve the weights and biases of the respective layer using algorithms called backpropagation and gradient descend, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6dbe149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_2L(W1: npt.NDArray, W2: npt.NDArray, B1: npt.ArrayLike, B2: npt.ArrayLike, training_data: npt.ArrayLike) -> tuple:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: Tuple of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    L1 = ReLU(W1.dot(training_data) + B1) # L1 shape is: rows = W1.rows, cols = training_data.cols.\n",
    "    L2 = softmax(W2.dot(L1) + B2)         # L2 shape is: rows = W2.rows (10 for digit classification), cols = training_data.cols.\n",
    "    return L1, L2\n",
    "\n",
    "def forward_prop_3L(W1: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, B1: npt.ArrayLike, B2: npt.ArrayLike, B3: npt.ArrayLike, training_data: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :W3: Third layer weight matrix.\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :B3: Third layer bias column vector.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: Tuple of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    Z1 = W1.dot(training_data) + B1 # Matrix multiplication of the training samples and first layer weights with addition of biases. The calculated matrix shape is: rows = W1.rows, cols = training_data.cols.\n",
    "    L1 = ReLU(Z1)                   # Nonlinear activation function applied on the first layer values. The shape of the resulting matrix is the stays the same.\n",
    "    Z2 = W2.dot(L1) + B2            # Matrix multiplication of the first layer values and second layer weights with addition of biases. The calculated matrix shape is: rows = W2.rows, cols = training_data.cols.\n",
    "    L2 = ReLU(Z2)                   # Nonlinear activation function applied on the second layer values. The shape of the resulting matrix is the stays the same.\n",
    "    Z3 = W3.dot(L2) + B3            # Matrix multiplication of the second layer values and third layer weights with addition of biases. The calculated matrix shape is: rows = W3.rows, cols = training_data.cols.\n",
    "    L3 = softmax(Z3)                # Transformation of the third layer outputs to a probability distribution. The shape of the resulting matrix is the stays the same.\n",
    "    return L1, L2, L3\n",
    "\n",
    "def forward_prop(weights: npt.NDArray, biases: npt.ArrayLike, activations: list, training_data: npt.NDArray) -> list:\n",
    "    \"\"\"\n",
    "    Classifies all training samples in to classes according to the given weights and biases.\n",
    "\n",
    "    :weights: List of weight matrices.\n",
    "    :biases: List of bias column vectors.\n",
    "    :activations: List of nonlinear activation functions.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :return: List of matrices with values for each layer.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    input = training_data\n",
    "    for i in range(len(weights)):\n",
    "        layers.append(activations[i](weights[i].dot(input) + biases[i])) # Matrix multiplication of the Nth layer and weights of the N+1th layer with addition of biases and aplication of nonlinear activation function.\n",
    "        input = layers[i]   # hold the current layer values for next iteration\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fdef0",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Backpropagation is an algorithm, which calculates derivatives of the loss function with respect to all weight and biases, i.e. calculating the gradients. The best lost function used in classification to muliple classes (digits in this case) is accoring to maximum likelihood the Cross-Entropy function.\n",
    "\n",
    "Given the example above the value of the Cross-Entropy function is caluclated as folows:\n",
    "$$\n",
    "\\mathbf{O} = softmax(\\mathbf{W_3} \\times ReLU(\\mathbf{W_2} \\times ReLU(\\mathbf{W_1} \\times \\mathbf{I} + B_1) + B_2) + B_3)\n",
    "$$\n",
    "$$\n",
    "loss = cross\\_entropy(\\mathbf{O}, \\mathbf{L}) =     \n",
    "    \\begin{bmatrix}\n",
    "        -\\sum_{n=0}^{i}{o_{n0} \\cdot log(l_{n0})} & -\\sum_{n=0}^{i}{o_{n1} \\cdot log(l_{n1})} & \\cdots & -\\sum_{n=0}^{i}{o_{nk} \\cdot log(l_{nk})}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "$\\mathbf{O}$ are the outputs of the neural network and $\\mathbf{L}$ are the labels with expected values, also known as the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a41a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_2L(L1: npt.NDArray, L2: npt.NDArray, W2: npt.NDArray, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :L1: Matrix of first layer values.\n",
    "    :L2: Matrix of second layer values.\n",
    "    :W2: Matrix of second layer weights.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of matrices with weight and bias gradients for each layer.\n",
    "    \"\"\"\n",
    "    m = one_hot_labels.shape[1] # The number of training samples. \n",
    "    dL2 = (L2 - one_hot_labels) / m   # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    dL1 = W2.T.dot(dL2) * ReLU_deriv(L1) # Back propagating the errors from output layer to the hidden layer by matrix multiplication of the transponed weight matrix and errors multiplied by the ReLU derivation. The calculated matrix shape is: rows: W2.cols == W1.rows (W2 is transposed), cols = one_hot_labels.cols (number of training samples).\n",
    "    dW2 = dL2.dot(L1.T)     # Matrix multiplication of the output layer errors and hidden layer values normalized by the number of training samples. The calculated matrix shape is: rows = one_hot_labels.rows == W2.rows (10 in case of digit classification), cols = L1.rows == W2.cols (L1 is transposed).\n",
    "    dB2 = np.sum(dL2, 1, keepdims=True)  # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dW1 = dL1.dot(training_data.T)       # Matrix multiplication of the hidden layer errors and input values normalized by the number of training samples. The calculated matrix shape is: rows = dL1.rows == W1.rows, cols = training_data.rows == W1.cols (training_data is transposed, in case of images 28x28 it is 784).\n",
    "    dB1 = np.sum(dL1, 1, keepdims=True)  # Arithmetic average of the hidden layer errors. The result is a scalar value.\n",
    "    return dW1, dW2, dB1, dB2\n",
    "\n",
    "def back_prop_3L(L1: npt.NDArray, L2: npt.NDArray, L3: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :L1: Matrix of first layer values.\n",
    "    :L2: Matrix of second layer values.\n",
    "    :L3: Matrix of third layer values.\n",
    "    :W2: Matrix of second layer weights.\n",
    "    :W3: Matrix of third layer weights.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of matrices with weight and bias gradients for each layer.\n",
    "    \"\"\"\n",
    "    m = one_hot_labels.shape[1]  # The number of training samples.\n",
    "    dL3 = (L3 - one_hot_labels) / m    # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    dL2 = W3.T.dot(dL3) * ReLU_deriv(L2) # Back propagating the errors from output layer to the last hidden layer by matrix multiplication of the transponed weight matrix and errors by the ReLU derivation. The calculated matrix shape is: rows: W3.cols == W2.rows (W3 is transposed), cols = one_hot_labels.cols (number of training samples).\n",
    "    dL1 = W2.T.dot(dL2) * ReLU_deriv(L1) # Back propagating the errors from last hidden layer to the first (second to last) hidden layer by matrix multiplication of the transponed weight matrix and errors by the ReLU derivation. The calculated matrix shape is: rows: W2.cols == W1.rows (W2 is transposed), cols = dL2.cols (number of training samples).\n",
    "    dW3 = dL3.dot(L2.T)      # Matrix multiplication of the output layer errors and hidden layer values normalized by the number of training samples. The calculated matrix shape is: rows = one_hot_labels.rows == W3.rows (10 in case of digit classification), cols = L2.rows == W3.cols (L2 is transposed).\n",
    "    dB3 = np.sum(dL3, 1, keepdims=True)        # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dW2 = dL2.dot(L1.T)      # Matrix multiplication of the last hidden layer errors and first layer values normalized by the number of training samples. The calculated matrix shape is: rows = dL2.rows == W2.rows, cols = L1.rows == W2.cols (L1 is transposed).\n",
    "    dB2 = np.sum(dL2, 1, keepdims=True)        # Arithmetic average of the errors. The result is a scalar value.\n",
    "    dW1 = dL1.dot(training_data.T)   # Matrix multiplication of the first hidden layer errors and input values normalized by the number of training samples. The calculated matrix shape is: rows = dL1.rows == W1.rows, cols = training_data.rows == W1.cols (training_data is transposed, in case of images 28x28 it is 784).\n",
    "    dB1 = np.sum(dL1, 1, keepdims=True)                # Arithmetic average of the errors. The result is a scalar value.\n",
    "    return dW1, dW2, dW3, dB1, dB2, dB3\n",
    "\n",
    "def back_prop(layers: list, weights: list, activation_derivs: list, training_data: npt.NDArray, one_hot_labels: npt.NDArray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates weight and bias gradients for all layers.\n",
    "\n",
    "    :layers: List of layer matrices.\n",
    "    :weights: List of weight matrices.\n",
    "    :activation_derivs: List of derivations of nonlinear activation functions.\n",
    "    :training_data: Training samples, each column represents one training sample.\n",
    "    :one_hot_labels: Encoded ground truth labels to probabilty distribution.\n",
    "    :return: Tuple of lists of matrices with weight and bias differences for each layer.\n",
    "    \"\"\"\n",
    "    layers_count = len(layers)                  # The number of layers.\n",
    "    m = one_hot_labels.shape[1]                 # The number of training samples.\n",
    "    weight_gradients = [None] * layers_count    # Initialization of weight gradients list\n",
    "    bias_diff = [None] * layers_count           # Initialization of bias differences list\n",
    "\n",
    "    current_deriv = (layers[layers_count - 1] - one_hot_labels) # Calculating errors with matrix subtractions between the classified values and the ground truth. Subtraction does not change the shape of the resulting matrix.\n",
    "    for i in range(layers_count - 1, 0, -1):  # Ranging from N to 1\n",
    "        weight_gradients[i] = current_deriv.dot(layers[i - 1].T)   # Matrix multiplication of the N+1th hidden layer errors and Nth hidden layer values normalized by the number of training samples. The shape of the resulting matrix changes with iterations, see above.\n",
    "        bias_diff[i] = np.sum(current_deriv)                       # Arithmetic average of the errors - scalar value.\n",
    "        current_deriv = weights[i].T.dot(current_deriv) * activation_derivs[i - 1](layers[i - 1]) # Back propagating the errors from N+1th layer to the Nth layer by matrix multiplication of the transponed weight matrix and errors by the derivation of activation function. The shape of the resulting matrix changes with iterations, see above.\n",
    "    \n",
    "    # Calculation for the first layer\n",
    "    weight_gradients[0] = current_deriv.dot(training_data.T)    # Matrix multiplication of the first hidden layer errors and input values normalized by the number of training samples.\n",
    "    bias_diff[0] = np.sum(current_deriv, 1, keepdims=True)      # Arithmetic average of the errors.\n",
    "\n",
    "    return weight_gradients, bias_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced063f",
   "metadata": {},
   "source": [
    "### Gradient Descend\n",
    "Gradient descend is an algorithm, which minimizes the error in classification of the neural network. It uses the backpropagation algorithm to obtain, how each weight and bias affect the error. By knowing how the weights and biases affect the error, they can be adjusted so that the error is smaller.\n",
    "\n",
    "The error is called loss function and the task in neural networks is to minimize the loss function. Minimizing means finding the global minimum of the loss function. On a (well-behaved) function ``f(x)`` with a single parametr ``x`` this can be done analytically by finding all ``x``, where the first derivation of the function is equal to 0, calculating the function for those ``x`` and selecting the smalles resulting ``y``.\n",
    "\n",
    "In neural networks the analytic expression of the loss function is not known and the function has multiple parameters (parameters are all the weights and biases). Finding the minimum analytically cannot be done, but it can be done numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e515b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend_2L(training_data: npt.NDArray, labels: npt.ArrayLike, L1_size: int, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network with 1 hidden layer using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    W1, W2 = weights_init_2L(L1_size)\n",
    "    B1, B2 = biases_init_2L(L1_size)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training\n",
    "    for _ in range(iterations):\n",
    "        L1, L2 = forward_prop_2L(W1, W2, B1, B2, training_data)\n",
    "        dW1, dW2, dB1, dB2 = back_prop_2L(L1, L2, W2, training_data, labels)\n",
    "        W1, W2 = weights_adjust_2L(W1, W2, dW1, dW2, learning_rate)\n",
    "        B1, B2 = biases_adjust_2L(B1, B2, dB1, dB2, learning_rate)\n",
    "\n",
    "    return W1, W2, B1, B2\n",
    "\n",
    "def gradient_descend_3L(training_data: npt.NDArray, labels: npt.ArrayLike, L1_size: int, L2_size: int, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network with 2 hidden layers using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :L2_size: Number of neurons in the second hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    W1, W2, W3 = weights_init_3L(L1_size, L2_size)\n",
    "    B1, B2, B3 = biases_init_3L(L1_size, L2_size)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training \n",
    "    for _ in range(iterations):\n",
    "        L1, L2, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, training_data)\n",
    "        dW1, dW2, dW3, dB1, dB2, dB3 = back_prop_3L(L1, L2, L3, W2, W3, training_data, labels)\n",
    "        W1, W2, W3 = weights_adjust_3L(W1, W2, W3, dW1, dW2, dW3, learning_rate)\n",
    "        B1, B2, B3 = biases_adjust_3L(B1, B2, B3, dB1, dB2, dB3, learning_rate)\n",
    "\n",
    "    return W1, W2, W3, B1, B2, B3\n",
    "\n",
    "def gradient_descend(training_data: npt.NDArray, labels: npt.ArrayLike, weight_sizes: list, activations: list, activation_derivs: list, iterations: int, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Trains a neural network given number of layers using gradient descend method.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :weigh_sizes: List of numbers of neurons in each layer including the input layer.\n",
    "    :activations: List of activation functions for each hidden layer and output layer.\n",
    "    :activation_derivs: List of derivation of activation functions each hidden layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of list of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    weights = weights_init(weight_sizes)\n",
    "    biases = biases_init(weight_sizes)\n",
    "    labels = one_hot(labels)\n",
    "\n",
    "    # training\n",
    "    for _ in range(iterations):\n",
    "        layers = forward_prop(weights, biases, activations, training_data)\n",
    "        weight_gradients, bias_diff = back_prop(layers, weights, activation_derivs, training_data, labels)\n",
    "        weights = weights_adjust(weights, weight_gradients, learning_rate)\n",
    "        biases = biases_adjust(biases, bias_diff, learning_rate)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabb76d",
   "metadata": {},
   "source": [
    "### Adjusting weights and biases after a training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6416852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_adjust_2L(W1: npt.NDArray, W2: npt.NDArray, dW1: npt.NDArray, dW2: npt.NDArray, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :dW1: First layer gradient matrix.\n",
    "    :dW2: Second layer gradient matrix.\n",
    "    :return: Tuple of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    return W1, W2\n",
    "\n",
    "def weights_adjust_3L(W1: npt.NDArray, W2: npt.NDArray, W3: npt.NDArray, dW1: npt.NDArray, dW2: npt.NDArray, dW3: npt.NDArray, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :W1: First layer weight matrix.\n",
    "    :W2: Second layer weight matrix.\n",
    "    :W2: Third layer weight matrix.\n",
    "    :dW1: First layer gradient matrix.\n",
    "    :dW2: Second layer gradient matrix.\n",
    "    :dW2: Third layer gradient matrix.\n",
    "    :return: Tuple of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    return W1, W2, W3\n",
    "\n",
    "def weights_adjust(weights: list, weight_gradients: list, learning_rate: float) -> list:\n",
    "    \"\"\"\n",
    "    Adjusts weights according to the gradients and learning rate.\n",
    "\n",
    "    :weights: List of weight matrices.\n",
    "    :weight_gradients: List of gradient matrices of the same shape as respective weight matrices.\n",
    "    :return: List of adjusted weight matrices.\n",
    "    \"\"\"\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - learning_rate * weight_gradients[i]\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def biases_adjust_2L(B1: npt.ArrayLike, B2: npt.ArrayLike, dB1: float, dB2: float, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the gradients and learning rate.\n",
    "\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :dB1: First layer bias difference (scalar value).\n",
    "    :dB2: Second layer bias difference (scalar value).\n",
    "    :return: Tuple of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "    return B1, B2\n",
    "\n",
    "def biases_adjust_3L(B1: npt.ArrayLike, B2: npt.ArrayLike, B3: npt.ArrayLike, dB1: float, dB2: float, dB3: float, learning_rate: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the difference and learning rate.\n",
    "\n",
    "    :B1: First layer bias column vector.\n",
    "    :B2: Second layer bias column vector.\n",
    "    :B2: Third layer bias column vector.\n",
    "    :dB1: First layer bias difference (scalar value).\n",
    "    :dB2: Second layer bias difference (scalar value).\n",
    "    :dB2: Third layer bias difference (scalar value).\n",
    "    :return: Tuple of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "    B3 = B3 - learning_rate * dB3\n",
    "    return B1, B2, B3\n",
    "\n",
    "def biases_adjust(biases: npt.ArrayLike, bias_diff: npt.ArrayLike, learning_rate: float) -> list:\n",
    "    \"\"\"\n",
    "    Adjusts biases according to the gradients and learning rate.\n",
    "\n",
    "    :biases: List of bias column vectors.\n",
    "    :bias_diff: List of bias differences (scalar values).\n",
    "    :return: List of adjusted bias column vectors.\n",
    "    \"\"\"\n",
    "    for i in range(len(biases)):\n",
    "        biases[i] = biases[i] - learning_rate * bias_diff[i]\n",
    "    \n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6c230",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97ebb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2L(training_data: npt.NDArray, training_labels: npt.ArrayLike, L1_size = L1_SIZE, iterations = ITERATIONS, learning_rate = LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains a neural network with one hidden layer using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :training_labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    W1, W2, B1, B2 = gradient_descend_2L(training_data, training_labels, L1_size, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    np.save(\"W1_2L.npy\", W1)\n",
    "    np.save(\"W2_2L.npy\", W2)\n",
    "    np.save(\"B1_2L.npy\", B1)\n",
    "    np.save(\"B2_2L.npy\", B2)\n",
    "\n",
    "def train_3L(training_data, training_labels, L1_size = L1_SIZE, L2_size = L2_SIZE, iterations = ITERATIONS, learning_rate = LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains a neural network with two hidden layers using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :training_labels: Ground truth values for each training sample.\n",
    "    :L1_size: Number of neurons in the first hidden neuron layer.\n",
    "    :L2_size: Number of neurons in the second hidden neuron layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    W1, W2, W3, B1, B2, B3 = gradient_descend_3L(training_data, training_labels, L1_size, L2_size, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    np.save(\"W1_3L.npy\", W1)\n",
    "    np.save(\"W2_3L.npy\", W2)\n",
    "    np.save(\"W3_3L.npy\", W3)\n",
    "    np.save(\"B1_3L.npy\", B1)\n",
    "    np.save(\"B2_3L.npy\", B2)\n",
    "    np.save(\"B3_3L.npy\", B3)\n",
    "\n",
    "def train(training_data: npt.NDArray, training_labels: npt.ArrayLike, weight_sizes: list, activations: list, activation_derivs: list, iterations = ITERATIONS, learning_rate = LEARNING_RATE, \n",
    "          files_id = dt.now().strftime(\"%d-%m-%Y_%H:%M:%S\")):\n",
    "    \"\"\"\n",
    "    Trains a neural network with a given number of hidden layers using gradient descend and saves the result.\n",
    "\n",
    "    :training_data: Traning samples organized into columns.\n",
    "    :labels: Ground truth values for each training sample.\n",
    "    :weigh_sizes: List of numbers of neurons in each layer including the input layer.\n",
    "    :activations: List of activation functions for each hidden layer and output layer.\n",
    "    :activation_derivs: List of derivation of activation functions each hidden layer.\n",
    "    :iterations: Number of training iterations.\n",
    "    :learning_rate: Learning rate used to adjust weights and biases.\n",
    "    :return: Tuple of list of weight and bias matrices.\n",
    "    \"\"\"\n",
    "    # training\n",
    "    weights, biases = gradient_descend(training_data, training_labels, weight_sizes, activations, activation_derivs, iterations, learning_rate)\n",
    "\n",
    "    # saving results\n",
    "    for i in range(len(weights)):\n",
    "        j = str(i + 1)\n",
    "        np.save(\"W\" + j + \"_\" + files_id + \".npy\", weights[i])\n",
    "        np.save(\"B\" + j + \"_\" + files_id + \".npy\", biases[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef8553",
   "metadata": {},
   "source": [
    "### Funtions for loading of training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6b3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\")\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    sample_count = training_data.shape[0]\n",
    "    training_data = np.reshape(training_data, (sample_count, -1)).T / 255\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\")\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    sample_count = test_data.shape[0]\n",
    "    test_data = np.reshape(test_data, (sample_count, -1)).T / 255\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4aab6",
   "metadata": {},
   "source": [
    "### Functions for result assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "432b605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_results_2L():\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    # Loading of the trained weights and biases.\n",
    "    W1 = np.load(\"W1_2L.npy\")\n",
    "    W2 = np.load(\"W2_2L.npy\")\n",
    "    B1 = np.load(\"B1_2L.npy\")\n",
    "    B2 = np.load(\"B2_2L.npy\")\n",
    "    print(\"\\n###################### 2 layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    _, L2 = forward_prop_2L(W1, W2, B1, B2, training_data)                 # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(L2, training_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    _, L2 = forward_prop_2L(W1, W2, B1, B2, test_data)             # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(L2, test_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(L2, test_labels, test_data)\n",
    "\n",
    "def assess_results_3L():\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with two hidden layer.\n",
    "    \"\"\"\n",
    "    # Loading of the trained weights and biases.\n",
    "    W1 = np.load(\"W1_3L.npy\")\n",
    "    W2 = np.load(\"W2_3L.npy\")\n",
    "    W3 = np.load(\"W3_3L.npy\")\n",
    "    B1 = np.load(\"B1_3L.npy\")\n",
    "    B2 = np.load(\"B2_3L.npy\")\n",
    "    B3 = np.load(\"B3_3L.npy\")\n",
    "    print(\"\\n###################### 3 layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    _, _, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, training_data)      # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(L3, training_labels)) # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    _, _, L3 = forward_prop_3L(W1, W2, W3, B1, B2, B3, test_data)   # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(L3, test_labels))  # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(L3, test_labels, test_data)\n",
    "\n",
    "def assess_results(activations: list, files_id = \"\"):\n",
    "    \"\"\"\n",
    "    Assesses the results of a neural network with given number of layers including the output layer.\n",
    "\n",
    "    :activations: Activation functions for each hidden layer and for the output layer.\n",
    "    :files_id: Id in the file names of the weights and biases.\n",
    "    \"\"\"\n",
    "    layers_count = len(activations)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    # Loading of the trained weights and biases.\n",
    "    for i in range(layers_count):\n",
    "        j = str(i + 1)\n",
    "        weights.append(np.load(\"W\" + j + \"_\" + files_id + \".npy\"))\n",
    "        biases.append(np.load(\"B\" + j + \"_\" + files_id + \".npy\"))\n",
    "    \n",
    "    print(\"\\n###################### \" + str(layers_count) + \" layers results ######################\\n\")\n",
    "\n",
    "    training_data, training_labels = load_training_data()\n",
    "    last_layer = forward_prop(weights, biases, activations, training_data)[layers_count - 1] # Calculation of the network output for each training sample.\n",
    "    print(\"Accuracy on training set: \", get_accuracy(last_layer, training_labels))           # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    test_data, test_labels = load_test_data()\n",
    "    last_layer = forward_prop(weights, biases, activations, test_data)[layers_count - 1]     # Calculation of the network output for each testing sample.\n",
    "    print(\"Accuracy on test set: \", get_accuracy(last_layer, test_labels))                   # Calculation of the accuracy of the network on training data.\n",
    "\n",
    "    show_some_mistakes(last_layer, test_labels, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f136be3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66870650",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "train_2L(training_data, training_labels)\n",
    "#train_3L(training_data, training_labels)\n",
    "#train(training_data, training_labels, [PIXELS_PER_IMAGE, 112, L1_SIZE, L2_SIZE, CLASSES_COUNT], [ReLU, ReLU, ReLU, softmax], \n",
    "#      [ReLU_deriv, ReLU_deriv, ReLU_deriv], files_id=\"4L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c77a9",
   "metadata": {},
   "source": [
    "### Result assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1789e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################### 2 layers results ######################\n",
      "\n",
      "Accuracy on training set:  0.905\n",
      "Accuracy on test set:  0.9073\n",
      "labeled: 6 -- classified: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 4 -- classified: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 7 -- classified: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 4 -- classified: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOZ0lEQVR4nO3da4wVdZrH8d8DDvECL+jVxVbcZXYiJsQLsyFosmYzZmDCGg1OEB0SjWaJPS+mI8bVFVgjGt14W1ZfGCb0OAK7mXWceNmRyUbGJUR3Y5zYXlbxwtBL6NAtNmGJ3Iwi8OyLLiY92PWv5tSpUwee7yfpnHPqOXXqSYUfVaeqTv3N3QXg1Deu7gYAtAZhB4Ig7EAQhB0IgrADQZzWyoWZGYf+gYq5u402vdSW3czmmdkWM+szs6VlPgtAtazR8+xmNl7S7yXNlTQg6S1Ji9z9o8Q8bNmBilWxZZ8tqc/dt7n7IUm/lDS/xOcBqFCZsJ8vaceI1wPZtD9iZl1m1mtmvSWWBaCkyg/QuXuPpB6J3XigTmW27IOSLhjxemo2DUAbKhP2tyRdaGbfNrMJkn4k6eXmtAWg2RrejXf3w2bWLWmDpPGSnnH3D5vWGYCmavjUW0ML4zs7ULlKLqoBcPIg7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIiGh2wGqnbZZZcl61999VWy/sknnzSznRMye/bsZH337t25tW3btjW7HUklw25m2yXtl3RE0mF3n9WMpgA0XzO27Fe5e/5/UwDaAt/ZgSDKht0l/dbM3jazrtHeYGZdZtZrZr0llwWghLK78Ve6+6CZ/amkV83sE3d/feQb3L1HUo8kmZmXXB6ABpXasrv7YPa4S9JLktKHIAHUpuGwm9lZZjbp2HNJP5C0uVmNAWiuMrvxUyS9ZGbHPuff3P2VpnSFU8b48eNza93d3cl5ly9fnqyffvrpyfr06dNza0NDQ8l5i9xzzz3J+sMPP5ysf/7557m1jo6ORloq1HDY3X2bpPRVDwDaBqfegCAIOxAEYQeCIOxAEIQdCIKfuKKUSZMmJetXXHFFbu3JJ59MznvkyJFk/bbbbkvWy5xemzlzZrJ+5513JusHDx5M1h944IETbak0tuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATn2ZFU9DPSm266KVlftWpVbu3w4cPJeefMmZOsv/baa8n6uHH527Jly5Yl57377ruT9aL1cvHFFyfrfX19yXoV2LIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCcZw9uxowZyfrKlSuT9Xnz5jW87DfffDNZLzqPXuTmm2/OrT300EPJeb/44otkfeHChcl6HefRi7BlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOM9+ikvdt10qvnf75ZdfXmr5GzduzK3deOONpT676N7ujz76aMOfXXRf9/Xr1zf82XUp3LKb2TNmtsvMNo+Y1mFmr5rZ1uxxcrVtAihrLLvxayUdf5nUUkkb3f1CSRuz1wDaWGHY3f11SXuOmzxf0rrs+TpJ1zW3LQDN1uh39inuvjN7/pmkKXlvNLMuSV0NLgdAk5Q+QOfubmaeqPdI6pGk1PsAVKvRU29DZtYpSdnjrua1BKAKjYb9ZUm3ZM9vkfTr5rQDoCrmnt6zNrNnJX1P0tmShiStkPTvkn4l6c8k9Uu6wd2PP4g32mexG1+B1Dnh7u7u5LwdHR3Jen9/f7L+xBNPJOtr1qzJre3bty8570UXXZSsr127NllPXWPwxhtvJOe96qqrkvVDhw4l63VydxtteuF3dndflFP6fqmOALQUl8sCQRB2IAjCDgRB2IEgCDsQROGpt6YujFNvo5o4cWKynjp9JUkLFizIrZmNehbmD7Zu3Zqs33777cn6K6+8kqynnHnmmcn6u+++m6xPnz49Wd+xY0dube7cucl5t2zZkqy3s7xTb2zZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIbiXdAp2dncn68uXLk/Xrr7++4WWvXr06WX/wwQeT9cHBwYaXLUnjxuVvT+69997kvEXn0Yuk1uvJfB69UWzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIzrM3wXnnnZesb9q0KVkvez75qaeeyq0tWbIkOe/Ro0dLLbtI6rf2y5YtK/XZAwMDyfrevXtLff6phi07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBefYxOu20/FW1YsWK5Lxlz6MXeeyxx3Jr1157bXLeomsEipxzzjnJ+tKlS0t9fsrUqVOT9VmzZuXW1q9f3+x22l7hlt3MnjGzXWa2ecS0+81s0Mzey/6urrZNAGWNZTd+raR5o0x/wt1nZn//0dy2ADRbYdjd/XVJe1rQC4AKlTlA121m72e7+ZPz3mRmXWbWa2a9JZYFoKRGw/5TSd+RNFPSTkkr897o7j3uPsvd84+WAKhcQ2F39yF3P+LuRyX9TNLs5rYFoNkaCruZjbw38g8lbc57L4D2UDg+u5k9K+l7ks6WNCRpRfZ6piSXtF3Sj919Z+HCTuLx2VPnZa+55poWdhLHkSNHkvXnn38+WV+8eHFu7eDBgw31dDLIG5+98KIad180yuSfl+4IQEtxuSwQBGEHgiDsQBCEHQiCsANB8BPXMZo2bVrdLeTav39/bq3oVtFnnHFGsj5hwoSGehqLnp6eZP3xxx9P1vv6+prZzimPLTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBFH4E9emLuwk/olr6rbERbdr7urqStbPPffcZL3ofPN9992XW/vyyy+T8z733HPJ+g033JCsF/n6669za1Wew48s7yeubNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAh+zz5Gvb35o1elapJ04MCBZD015LIkzZkzJ1lfvXp1bu3WW29Nzrtw4cJkHacOtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATn2VtgzZo1yfoll1ySrM+dOzdZb+f7p2/YsKHuFpAp3LKb2QVmtsnMPjKzD81sSTa9w8xeNbOt2ePk6tsF0Kix7MYflvR37j5D0hWSfmJmMyQtlbTR3S+UtDF7DaBNFYbd3Xe6+zvZ8/2SPpZ0vqT5ktZlb1sn6bqKegTQBCf0nd3Mpkn6rqTfSZri7juz0meSpuTM0yUpfRM2AJUb89F4M5so6QVJd7j7vpE1H75r5ag3k3T3Hnef5e75d2wEULkxhd3MvqXhoP/C3V/MJg+ZWWdW75S0q5oWATRD4a2kzcw0/J18j7vfMWL645L+z90fMbOlkjrc/e8LPuukvZV0nRYsWJCsr127Nrc2ceLEUsv+9NNPk/WBgYFkPXUr6v7+/oZ6QlreraTH8p39ryTdLOkDM3svm7Zc0iOSfmVmiyX1Syp3g3EAlSoMu7v/t6RR/6eQ9P3mtgOgKlwuCwRB2IEgCDsQBGEHgiDsQBD8xPUkMDg4mKyXGfr4rrvuStaffvrpZH3v3r0NLxutxZYdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4LgPPtJ4NJLL03Wt2zZkltbtWpVct7UcM+SVHS/A5w82LIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCF941v6sK4bzxQubz7xrNlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCsNuZheY2SYz+8jMPjSzJdn0+81s0Mzey/6urr5dAI0qvKjGzDoldbr7O2Y2SdLbkq7T8HjsB9z9n8a8MC6qASqXd1HNWMZn3ylpZ/Z8v5l9LOn85rYHoGon9J3dzKZJ+q6k32WTus3sfTN7xswm58zTZWa9ZtZbrlUAZYz52ngzmyjpNUn/6O4vmtkUSbsluaQHNbyr/7cFn8FuPFCxvN34MYXdzL4l6TeSNrj7P49SnybpN+5+ccHnEHagYg3/EMbMTNLPJX08MujZgbtjfihpc9kmAVRnLEfjr5T0X5I+kHQ0m7xc0iJJMzW8G79d0o+zg3mpz2LLDlSs1G58sxB2oHr8nh0IjrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBE4Q0nm2y3pP4Rr8/OprWjdu2tXfuS6K1Rzeztz/MKLf09+zcWbtbr7rNqayChXXtr174kemtUq3pjNx4IgrADQdQd9p6al5/Srr21a18SvTWqJb3V+p0dQOvUvWUH0CKEHQiilrCb2Twz22JmfWa2tI4e8pjZdjP7IBuGutbx6bIx9HaZ2eYR0zrM7FUz25o9jjrGXk29tcUw3olhxmtdd3UPf97y7+xmNl7S7yXNlTQg6S1Ji9z9o5Y2ksPMtkua5e61X4BhZn8t6YCkfzk2tJaZPSZpj7s/kv1HOdnd72mT3u7XCQ7jXVFvecOM36oa110zhz9vRB1b9tmS+tx9m7sfkvRLSfNr6KPtufvrkvYcN3m+pHXZ83Ua/sfScjm9tQV33+nu72TP90s6Nsx4resu0VdL1BH28yXtGPF6QO013rtL+q2ZvW1mXXU3M4opI4bZ+kzSlDqbGUXhMN6tdNww422z7hoZ/rwsDtB905Xu/peS/kbST7Ld1bbkw9/B2unc6U8lfUfDYwDulLSyzmayYcZfkHSHu+8bWatz3Y3SV0vWWx1hH5R0wYjXU7NpbcHdB7PHXZJe0vDXjnYydGwE3exxV839/IG7D7n7EXc/KulnqnHdZcOMvyDpF+7+Yja59nU3Wl+tWm91hP0tSRea2bfNbIKkH0l6uYY+vsHMzsoOnMjMzpL0A7XfUNQvS7ole36LpF/X2MsfaZdhvPOGGVfN66724c/dveV/kq7W8BH5/5X0D3X0kNPXX0j6n+zvw7p7k/SshnfrvtbwsY3Fkv5E0kZJWyX9p6SONurtXzU8tPf7Gg5WZ029XanhXfT3Jb2X/V1d97pL9NWS9cblskAQHKADgiDsQBCEHQiCsANBEHYgCMIOBEHYgSD+HzZ8de5T/ypTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 6 -- classified: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 6 -- classified: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 0 -- classified: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 9 -- classified: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 7 -- classified: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN50lEQVR4nO3db6wV9Z3H8c9XARNvMYFeJUhx2200pv5Z2NzAJks2XaUN3CfQmJjyYMPGxttIXYv0gQYfVJ+RzdK60aTJRUzpptqQFBUT/xRIE3dDJF4Ii6BSWYPyn0XRgolWrt99cAf3ind+czkzc2Yu3/cruTnnzPfMmW9GP8ycM39+5u4CcOm7rOkGAHQHYQeCIOxAEIQdCIKwA0FM6ubCzIyf/oGaubuNNb3Ult3MFpnZfjM7YGYPlvksAPWyTo+zm9nlkv4k6XuSDkt6TdIyd38jMQ9bdqBmdWzZ50k64O7vuPtfJP1O0pISnwegRmXCPkvSoVGvD2fTvsTMBsxsyMyGSiwLQEm1/0Dn7oOSBiV244EmldmyH5E0e9Trb2TTALRQmbC/Jul6M/uWmU2R9ENJm6tpC0DVOt6Nd/dzZnavpJclXS7pSXffV1lnACrV8aG3jhbGd3agdrWcVANg4iDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBNHx+OySZGYHJZ2RNCzpnLv3VdEUgOqVCnvmH939VAWfA6BG7MYDQZQNu0v6g5ntNLOBsd5gZgNmNmRmQyWXBaAEc/fOZzab5e5HzOwaSVsk/Yu7v5J4f+cLAzAu7m5jTS+1ZXf3I9njSUnPSJpX5vMA1KfjsJtZj5lNPf9c0vcl7a2qMQDVKvNr/AxJz5jZ+c95yt1fqqQrTBhXXXVVsj537tzcWn9/f6llT506NVmfPHlybm3FihXJeT/77LOOemqzjsPu7u9I+psKewFQIw69AUEQdiAIwg4EQdiBIAg7EESpM+guemGcQdc6U6ZMSdZXrVqVrK9cuTJZv+aaay62pa5YvHhxsv7yyy93qZPq1XIGHYCJg7ADQRB2IAjCDgRB2IEgCDsQBGEHgqjihpMo6eabb07Wr7zyymT9/vvvz62lLvOUii9RXbhwYbKOiYMtOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXH2CsyZMydZL7rm+4477kjWe3p6LrKj7tmxY0ey/uGHH+bWjh8/npx3+fLlnbSEHGzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIjrNnrrjiimR9cHAwt7ZkyZLkvEXXjNdp165dyfr+/fuT9U2bNiXrL72UHqX7448/zq2l1mkVnn322dza9u3ba112GxVu2c3sSTM7aWZ7R02bbmZbzOzt7HFavW0CKGs8u/G/lrTogmkPStrm7tdL2pa9BtBihWF391ckfXDB5CWSNmTPN0haWm1bAKrW6Xf2Ge5+LHt+XNKMvDea2YCkgQ6XA6AipX+gc3dPDdjo7oOSBiUGdgSa1OmhtxNmNlOSsseT1bUEoA6dhn2zpPPXHy6X9Fw17QCoS+H47Gb2tKTvSuqVdELSzyU9K2mjpOskvSvpTne/8Ee8sT6rtbvxRdeMv/rqq7m1m266KTnve++9l6w///zzyXrRsfKtW7fm1lLXk0vSmTNnkvWybrzxxtxa0bXwU6dOTdbPnj2brN922225taGhoeS8E1ne+OyF39ndfVlO6fZSHQHoKk6XBYIg7EAQhB0IgrADQRB2IAgucc2kLsWUpL6+vtzapEnp1Tg8PJysf/LJJ8l6m112WXp7sWrVqtxa0aG1IqdPn07W9+zZk1t76KGHkvNee+21yfr06dOT9XvuuSdZLzokWge27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQROElrpUurMWXuKIzjz76aLJ+3333daeRi1R0jH7dunXJ+po1a5L1Jo6jn5d3iStbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IguvZLwFXX311RzVJ6u3tTdbvuuuuZH3x4sXJehlF54C89dZbyfrmzZtza4899lhy3qNHjybrExFbdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IguPsmcmTJyfr8+fPz60tXbo0Oe/OnTuT9YULFybrRcfKb7jhho5q42E25qXRXxjHkN+5tccffzw57wsvvJCsv/jii8k6vqxwy25mT5rZSTPbO2raw2Z2xMx2Z3/99bYJoKzx7Mb/WtKiMab/0t3nZH/pf4IBNK4w7O7+iqQPutALgBqV+YHuXjPbk+3mT8t7k5kNmNmQmQ2VWBaAkjoN+68kfVvSHEnHJK3Ne6O7D7p7n7vnj4wIoHYdhd3dT7j7sLt/LmmdpHnVtgWgah2F3cxmjnr5A0l7894LoB0K7xtvZk9L+q6kXkknJP08ez1Hkks6KOnH7n6scGEtvm980Xjbp06d6lInl5bDhw/n1m699dbkvE3ee30iy7tvfOFJNe6+bIzJ60t3BKCrOF0WCIKwA0EQdiAIwg4EQdiBILjENfPRRx8l6wsWLMitPfXUU8l5r7vuuo56aoOiS1wPHTqUrPf3518QyaG17mLLDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBcJw9Mzw8nKxv3749t7Zo0Vj34/x/d999d7JeNPTwrFmzkvXVq1fn1iZNKvefuOgS6HXr1iXre/dyq4O2YMsOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EU3kq60oW1+FbSbTZlypRk/f3338+t9fT0lFr2gQMHkvXbb789WS+63h3Vy7uVNFt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiC69kngI0bNybrZY6lnzt3LllfsWJFss5x9ImjcMtuZrPN7I9m9oaZ7TOzn2bTp5vZFjN7O3ucVn+7ADo1nt34c5J+5u7fkfR3kn5iZt+R9KCkbe5+vaRt2WsALVUYdnc/5u67sudnJL0paZakJZI2ZG/bIGlpTT0CqMBFfWc3s29Kmitph6QZ7n4sKx2XNCNnngFJAyV6BFCBcf8ab2Zfk/R7SSvd/c+jaz5yNc2YF7m4+6C797l7X6lOAZQyrrCb2WSNBP237r4pm3zCzGZm9ZmSTtbTIoAqFO7G28iYveslvenuvxhV2ixpuaQ12eNztXQYwLRp6QMZ8+fPr23Za9euTda3bt1a27LRXeP5zv73kv5J0utmtjubtlojId9oZj+S9K6kO2vpEEAlCsPu7v8lacyL4SWl71wAoDU4XRYIgrADQRB2IAjCDgRB2IEguJV0F/T1pU8eXL9+fbJ+yy23dLzs06dPl/rso0ePdrxsNINbSQPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAENxKugt6e3uT9TLH0SUpda7EAw88kJyX4+hxsGUHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSA4zn4JSA27/MQTT3SxE7QZW3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCGI847PPlvQbSTMkuaRBd/93M3tY0t2S/jd762p3f6GuRieybdu2Jev79u1L1j/99NNk/ZFHHrnonhDPeE6qOSfpZ+6+y8ymStppZluy2i/d/d/qaw9AVcYzPvsxScey52fM7E1Js+puDEC1Luo7u5l9U9JcSTuySfea2R4ze9LMpuXMM2BmQ2Y2VK5VAGWMO+xm9jVJv5e00t3/LOlXkr4taY5Gtvxrx5rP3Qfdvc/d0wOeAajVuMJuZpM1EvTfuvsmSXL3E+4+7O6fS1onaV59bQIoqzDsZmaS1kt6091/MWr6zFFv+4GkvdW3B6AqhUM2m9kCSf8p6XVJn2eTV0tappFdeJd0UNKPsx/zUp8VcshmoJvyhmxmfHbgEsP47EBwhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSC6PWTzKUnvjnrdm01ro7b21ta+JHrrVJW9/VVeoavXs39l4WZDbb03XVt7a2tfEr11qlu9sRsPBEHYgSCaDvtgw8tPaWtvbe1LordOdaW3Rr+zA+ieprfsALqEsANBNBJ2M1tkZvvN7ICZPdhED3nM7KCZvW5mu5seny4bQ++kme0dNW26mW0xs7ezxzHH2Guot4fN7Ei27nabWX9Dvc02sz+a2Rtmts/MfppNb3TdJfrqynrr+nd2M7tc0p8kfU/SYUmvSVrm7m90tZEcZnZQUp+7N34Chpn9g6Szkn7j7jdn0/5V0gfuvib7h3Kauz/Qkt4elnS26WG8s9GKZo4eZlzSUkn/rAbXXaKvO9WF9dbEln2epAPu/o67/0XS7yQtaaCP1nP3VyR9cMHkJZI2ZM83aOR/lq7L6a0V3P2Yu+/Knp+RdH6Y8UbXXaKvrmgi7LMkHRr1+rDaNd67S/qDme00s4GmmxnDjFHDbB2XNKPJZsZQOIx3N10wzHhr1l0nw5+XxQ90X7XA3f9W0mJJP8l2V1vJR76DtenY6biG8e6WMYYZ/0KT667T4c/LaiLsRyTNHvX6G9m0VnD3I9njSUnPqH1DUZ84P4Ju9niy4X6+0KZhvMcaZlwtWHdNDn/eRNhfk3S9mX3LzKZI+qGkzQ308RVm1pP9cCIz65H0fbVvKOrNkpZnz5dLeq7BXr6kLcN45w0zrobXXePDn7t71/8k9WvkF/n/kfRQEz3k9PXXkv47+9vXdG+SntbIbt1nGvlt40eSvi5pm6S3JW2VNL1Fvf2HRob23qORYM1sqLcFGtlF3yNpd/bX3/S6S/TVlfXG6bJAEPxABwRB2IEgCDsQBGEHgiDsQBCEHQiCsANB/B+K/Vs02W0PugAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled: 8 -- classified: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assess_results_2L()\n",
    "#assess_results_3L()\n",
    "#assess_results([ReLU, ReLU, ReLU, softmax], \"4L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a01af5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO dropout\n",
    "pKeep = 0.8\n",
    "weights = np.ones([5, 5])\n",
    "binary_value = np.random.rand(weights.shape[0], weights.shape[1]) < pKeep\n",
    "res = np.multiply(weights, binary_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
