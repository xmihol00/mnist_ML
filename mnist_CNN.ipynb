{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "import scipy.signal as sps\n",
    "import string as str\n",
    "\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1\n",
    "\n",
    "def random_name():\n",
    "    return \"\".join(rnd.choices(str.ascii_letters + str.digits, k=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, L)\n",
    "\n",
    "def ReLU_deriv(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return L > 0\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :L: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    return np.exp(L) / sum(np.exp(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\") / 255\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\") / 255\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward():\n",
    "        pass\n",
    "\n",
    "    def adjust():\n",
    "        pass\n",
    "\n",
    "    def backward():\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            np.save(self.name + \"_K.npy\", self.kernels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_W.npy\", self.weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_B.npy\", self.biases)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def load(self, kernels: bool, weights: bool, biases: bool):\n",
    "        if kernels:\n",
    "            try:\n",
    "                self.kernels = np.load(self.name + \"_K.npy\")\n",
    "            except:\n",
    "                kernels = False\n",
    "        \n",
    "        if weights:\n",
    "            try:\n",
    "                self.weights = np.load(self.name + \"_W.npy\")\n",
    "            except:\n",
    "                weights = False\n",
    "        \n",
    "        if biases:\n",
    "            try:\n",
    "                self.biases = np.load(self.name + \"_B.npy\")\n",
    "            except:\n",
    "                biases = False\n",
    "        \n",
    "        return kernels, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Layer):\n",
    "    def __init__(self, kernel_count, kernel_size, activation, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        kernels, _, biases = self.load(True, False, True)\n",
    "        if not kernels:\n",
    "            self.kernels = np.random.rand(kernel_count, kernel_size, kernel_size) - 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.random.rand(kernel_count) - 0.5\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        kernel_count = self.kernels.shape[0]\n",
    "        input_count = input.shape[0]\n",
    "        output = np.zeros((input_count * kernel_count, input.shape[1] - 2, input.shape[2] - 2))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            k = i * kernel_count\n",
    "            for j in range(kernel_count):\n",
    "                output[k + j] = self.activation(sps.correlate2d(input[i], self.kernels[j], \"valid\") + self.biases[j])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, size, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input_count = input.shape[0]\n",
    "        pooled_width = int(input.shape[1] / self.size)\n",
    "        pooled_height = int(input.shape[2] / self.size)\n",
    "        pooled = np.zeros((input_count, pooled_width, pooled_height))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            for j in range(pooled_width):\n",
    "                for k in range(pooled_height):\n",
    "                    l = j * self.size\n",
    "                    m = k * self.size\n",
    "                    pooled[i, j, k] = np.max(input[i, l:l+self.size, m:m+self.size])\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def adjust():\n",
    "        #np.argmax(window, axis=0, keepdims=True,)[0]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(Layer):\n",
    "    def __init__(self, sample_count, channel_count, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.sample_count = sample_count\n",
    "        self.channel_count = channel_count\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, (self.sample_count, input.shape[1] * input.shape[2] * self.channel_count)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        _, weights, biases = self.load(False, True, True)\n",
    "        if not weights:\n",
    "            self.weights = np.random.rand(output_size, input_size) - 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.random.rand(output_size, 1) - 0.5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.weights.dot(input) + self.biases)\n",
    "    \n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        self.weights = self.weights - (dOutput.dot(self.input.T) / sample_count) * learning_rate\n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "    \n",
    "    def backward(self, dOutput):\n",
    "        return self.weights.T.dot(dOutput) * self.activation_deriv(self.input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_data, training_labels, *layers: Layer):\n",
    "        self.training_data = training_data\n",
    "        self.sample_count = training_data.shape[0]\n",
    "        self.training_labels = training_labels\n",
    "        self.one_hot_training_labels = one_hot(training_labels)\n",
    "        self.layers = layers\n",
    "    \n",
    "    def train(self, iterations, learning_rate):\n",
    "        for _ in range(iterations): \n",
    "            input = self.training_data\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "\n",
    "            dOutput = input - self.one_hot_training_labels\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                self.layers[i].adjust(dOutput, self.sample_count, learning_rate)\n",
    "                dOutput = self.layers[i].backward(dOutput)\n",
    "\n",
    "            self.layers[0].adjust(dOutput, self.sample_count, learning_rate)\n",
    "        \n",
    "    def save(self):\n",
    "        for layer in self.layers:\n",
    "            layer.save()\n",
    "\n",
    "    def assess(self, test_data, test_labels, on_trainig_set = True):\n",
    "        input = test_data\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax(input, 0) == test_labels) / test_labels.size)\n",
    "        print(\"\\n############################# Neural Network Results #############################\\n\")\n",
    "        print(\"Accuracy on test set: \", accuracy)\n",
    "\n",
    "        if on_trainig_set:\n",
    "            input = self.training_data\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "            \n",
    "            accuracy = (np.sum(np.argmax(input, 0) == self.training_labels) / self.training_labels.size)\n",
    "            print(\"Accuracy on training set: \", accuracy)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4102/2070122192.py:35: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(L) / sum(np.exp(L))\n",
      "/tmp/ipykernel_4102/2070122192.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(L) / sum(np.exp(L))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################# Neural Network Results #############################\n",
      "\n",
      "Accuracy on test set:  0.09871666666666666\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "sample_count = training_data.shape[0]\n",
    "training_data = np.reshape(training_data, (sample_count, -1)).T\n",
    "\n",
    "neural_network = NeuralNetwork(training_data, training_labels, FullyConnectedLayer(784, 56, ReLU, None, \"FCL1\"), FullyConnectedLayer(56, 10, softmax, ReLU_deriv, \"OL\"))\n",
    "neural_network.train(50, 0.15)\n",
    "neural_network.assess(training_data, training_labels, False)\n",
    "\n",
    "#conv_layer1 = ConvolutionLayer(2, 3, ReLU)\n",
    "#conv_layer2 = ConvolutionLayer(2, 3, ReLU)\n",
    "#max_pool_layer = MaxPoolLayer(2)\n",
    "#flat_layer = FlatteningLayer(2, 4)\n",
    "#\n",
    "#output = conv_layer1.forward(training_data[0:2])\n",
    "#output = conv_layer2.forward(output)\n",
    "#output = max_pool_layer.forward(output)\n",
    "#flat = flat_layer.forward(output)\n",
    "#\n",
    "#for i in output:\n",
    "#    plt.imshow(i, cmap='gray')\n",
    "#    plt.show()\n",
    "#\n",
    "#print(flat.shape)\n",
    "#plt.imshow(np.reshape(flat[0:144, 0], (12, 12)), cmap='gray')\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
