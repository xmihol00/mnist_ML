{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "import scipy.signal as sps\n",
    "import string as str\n",
    "\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1\n",
    "\n",
    "def random_name():\n",
    "    return \"\".join(rnd.choices(str.ascii_letters + str.digits, k=8))\n",
    "\n",
    "def argmax2D(matrix):\n",
    "    x, y = 0, 0\n",
    "    for i in range(1, matrix.shape[0]):\n",
    "        for j in range(1, matrix.shape[1]):\n",
    "            if matrix[i, j] > matrix[x, y]:\n",
    "                x, y = i, j\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def leaky_ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0.1 * X, X)\n",
    "\n",
    "def ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :X: Matrix of values.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return X > 0\n",
    "\n",
    "def leaky_ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.where(X > 0, 1, 0.1)\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :X: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    #print(X[:,0])\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\") / 255\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\") / 255\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        pass\n",
    "\n",
    "    def add_network_name(self, name):\n",
    "        self.network_name = name\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            np.save(f\"{self.network_name}/{self.name}_K.npy\", self.kernels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(f\"{self.network_name}/{self.name}_W.npy\", self.weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(f\"{self.network_name}/{self.name}_B.npy\", self.biases)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def load(self, kernels: bool, weights: bool, biases: bool):\n",
    "        if kernels:\n",
    "            try:\n",
    "                self.kernels = np.load(f\"{self.network_name}/{self.name}_K.npy\")\n",
    "            except:\n",
    "                kernels = False\n",
    "        \n",
    "        if weights:\n",
    "            try:\n",
    "                self.weights = np.load(f\"{self.network_name}/{self.name}_W.npy\")\n",
    "            except:\n",
    "                weights = False\n",
    "        \n",
    "        if biases:\n",
    "            try:\n",
    "                self.biases = np.load(f\"{self.network_name}/{self.name}_B.npy\")\n",
    "            except:\n",
    "                biases = False\n",
    "        \n",
    "        return kernels, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Layer):\n",
    "    def __init__(self, kernel_count, kernel_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        self.kernel_count = kernel_count\n",
    "        self.kernel_shrink = kernel_size - 1\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def load(self):\n",
    "        kernels, _, biases = super().load(True, False, True)\n",
    "        if not kernels:\n",
    "            lower, upper = -(1.0 / self.kernel_size), (1.0 / self.kernel_size)\n",
    "            self.kernels = lower + np.random.randn(self.kernel_count, self.kernel_size, self.kernel_size) * (upper - lower)\n",
    "        if not biases:\n",
    "            self.biases = np.zeros((self.kernel_count, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        input_count = input.shape[0]\n",
    "        output = np.zeros((input_count * self.kernel_count, input.shape[1] - self.kernel_shrink, input.shape[2] - self.kernel_shrink))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                output[k + j] = self.activation(sps.correlate2d(input[i], self.kernels[j], \"valid\") + self.biases[j])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        new_dOutput = None\n",
    "        if self.activation_deriv is not None:\n",
    "            new_dOutput = np.zeros_like(self.input)\n",
    "            rotated_kernels = np.rot90(self.kernels, 2, (1, 2))\n",
    "            for i in range(self.input.shape[0]):\n",
    "                k = i * self.kernel_count\n",
    "                for j in range(self.kernel_count):\n",
    "                    new_dOutput[i] += sps.correlate2d(rotated_kernels[j], dOutput[k + j], \"full\")\n",
    "                    \n",
    "            new_dOutput = new_dOutput * self.activation_deriv(self.input)\n",
    "\n",
    "        dKernels = np.zeros_like(self.kernels)\n",
    "        for i in range(sample_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                dKernels[j] += sps.correlate2d(self.input[i], dOutput[k + j], \"valid\")\n",
    "        \n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "        self.kernels = self.kernels - (dKernels / sample_count) * learning_rate\n",
    "\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, window_size, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def load(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.input_count = input.shape[0]\n",
    "        self.pooled_width = int(input.shape[1] / self.window_size)\n",
    "        self.pooled_height = int(input.shape[2] / self.window_size)\n",
    "        pooled = np.zeros((self.input_count, self.pooled_width, self.pooled_height))\n",
    "        \n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    pooled[i, j, k] = np.max(input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "    def backward(self, dOutput, *_):\n",
    "        new_dOutput = np.zeros_like(self.input)\n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    x, y = argmax2D(self.input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "                    x += l\n",
    "                    y += m\n",
    "                    new_dOutput[i, x, y] = dOutput[i, j, k]\n",
    "\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(Layer):\n",
    "    def __init__(self, channel_count, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.channel_count = channel_count\n",
    "    \n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        return np.reshape(input, (-1, input.shape[1] * input.shape[2] * self.channel_count)).T\n",
    "    \n",
    "    def backward(self, dOutput, *_):\n",
    "        return np.reshape(dOutput.T, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation_deriv = activation_deriv\n",
    "        self.activation = activation\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def load(self):\n",
    "        _, weights, biases = super().load(False, True, True)\n",
    "        if not weights:\n",
    "            self.weights = np.random.randn(self.output_size, self.input_size) * np.sqrt(2 / self.input_size)\n",
    "        if not biases:\n",
    "            self.biases = np.zeros((self.output_size, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.weights.dot(input) + self.biases)\n",
    "    \n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        new_dOutput = None\n",
    "        if self.activation_deriv is not None:\n",
    "            new_dOutput = self.weights.T.dot(dOutput) * self.activation_deriv(self.input)\n",
    "            \n",
    "        self.weights = self.weights - (dOutput.dot(self.input.T) / sample_count) * learning_rate\n",
    "        self.biases = self.biases - (np.sum(dOutput, 0) / sample_count) * learning_rate\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_data, training_labels, name = random_name(), *layers: Layer):\n",
    "        self.training_data = training_data\n",
    "        self.training_data_count = training_data.shape[0]\n",
    "        self.training_labels = training_labels\n",
    "        self.name = name\n",
    "        self.layers = layers\n",
    "        for layer in layers:\n",
    "            layer.add_network_name(name)\n",
    "    \n",
    "    def load(self):\n",
    "        for layer in self.layers:\n",
    "            layer.load()\n",
    "    \n",
    "    def train(self, batch_size, iterations, learning_rate):\n",
    "        batch_index = batch_size\n",
    "        for _ in range(iterations):\n",
    "            input = self.training_data[batch_index - batch_size : batch_index]\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "\n",
    "            dOutput = input - self.training_labels[:, batch_index - batch_size : batch_index]\n",
    "            for i in range(len(self.layers) - 1, -1, -1):\n",
    "                dOutput = self.layers[i].backward(dOutput, batch_size, learning_rate)\n",
    "\n",
    "            batch_index += batch_size\n",
    "            if batch_index > self.training_data_count:\n",
    "                batch_index = batch_size\n",
    "        \n",
    "    def save(self):\n",
    "        for layer in self.layers:\n",
    "            layer.save()\n",
    "\n",
    "    def assess(self, test_data, test_labels, text):\n",
    "        input = test_data\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax(input, 0) == test_labels) / test_labels.size)\n",
    "        print(f\"\\n############################# Neural Network '{self.name}' Results #############################\")\n",
    "        print(f\"Accuracy {text}:\", accuracy, \"({:.2f}%)\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "test_data, test_labels = load_test_data()\n",
    "\n",
    "def neural_network_1(name = \"neural_network_1\"):\n",
    "    return NeuralNetwork(training_data, one_hot(training_labels), name,\n",
    "                         ConvolutionLayer(2, 3, leaky_ReLU, None, \"CL1\"),\n",
    "                         ConvolutionLayer(2, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL2\"),\n",
    "                         ConvolutionLayer(2, 5, leaky_ReLU, leaky_ReLU_deriv, \"CL3\"),\n",
    "                         FlatteningLayer(8, \"FL1\"), \n",
    "                         FullyConnectedLayer(3200, 80, ReLU, leaky_ReLU_deriv, \"FCL1\"), \n",
    "                         FullyConnectedLayer(80, 10, softmax, ReLU_deriv, \"OL\"))\n",
    "\n",
    "def neural_network_2(name = \"neural_network_2\"):\n",
    "    return NeuralNetwork(training_data, one_hot(training_labels), name,\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, None, \"CL1\"),\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL2\"),\n",
    "                         ConvolutionLayer(2, 5, leaky_ReLU, leaky_ReLU_deriv, \"CL3\"),\n",
    "                         FlatteningLayer(18, \"FL1\"), \n",
    "                         FullyConnectedLayer(7200, 400, leaky_ReLU, leaky_ReLU_deriv, \"FCL1\"),\n",
    "                         FullyConnectedLayer(400, 80, ReLU, leaky_ReLU_deriv, \"FCL2\"), \n",
    "                         FullyConnectedLayer(80, 10, softmax, ReLU_deriv, \"OL\"))\n",
    "\n",
    "def neural_network_3(name = \"neural_network_3\"):\n",
    "    return NeuralNetwork(training_data, one_hot(training_labels), name,\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, None, \"CL1\"),\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL2\"),\n",
    "                         MaxPoolLayer(2, \"MPL1\"),\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL3\"),\n",
    "                         ConvolutionLayer(3, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL4\"),\n",
    "                         FlatteningLayer(81, \"FL1\"), \n",
    "                         FullyConnectedLayer(5184, 750, leaky_ReLU, leaky_ReLU_deriv, \"FCL1\"),\n",
    "                         FullyConnectedLayer(750, 10, softmax, leaky_ReLU_deriv, \"OL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = neural_network_1()\n",
    "nn1.load()\n",
    "nn1.train(batch_size=100, iterations=1, learning_rate=0.03)\n",
    "nn1.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################# Neural Network 'neural_network_1' Results #############################\n",
      "Accuracy on training set: 0.9674 (96.74%)\n",
      "\n",
      "############################# Neural Network 'neural_network_1' Results #############################\n",
      "Accuracy on test set: 0.9641 (96.41%)\n"
     ]
    }
   ],
   "source": [
    "nn1 = neural_network_1()\n",
    "nn1.load()\n",
    "nn1.assess(training_data, training_labels, \"on training set\")\n",
    "nn1.assess(test_data, test_labels, \"on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2 = neural_network_2()\n",
    "nn2.load()\n",
    "nn2.train(batch_size=50, iterations=1, learning_rate=0.01)\n",
    "nn2.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################# Neural Network 'neural_network_2' Results #############################\n",
      "Accuracy on training set: 0.9880666666666666 (98.81%)\n",
      "\n",
      "############################# Neural Network 'neural_network_2' Results #############################\n",
      "Accuracy on test set: 0.9773 (97.73%)\n"
     ]
    }
   ],
   "source": [
    "nn2 = neural_network_2()\n",
    "nn2.load()\n",
    "nn2.assess(training_data, training_labels, \"on training set\")\n",
    "nn2.assess(test_data, test_labels, \"on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn3 = neural_network_3()\n",
    "nn3.load()\n",
    "nn3.train(batch_size=100, iterations=600, learning_rate=0.02)\n",
    "nn3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################# Neural Network 'neural_network_3' Results #############################\n",
      "Accuracy on test set: 0.9319 (93.19%)\n"
     ]
    }
   ],
   "source": [
    "nn3 = neural_network_3()\n",
    "nn3.load()\n",
    "#nn3.assess(training_data, training_labels, \"on training set\")\n",
    "nn3.assess(test_data, test_labels, \"on test set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
