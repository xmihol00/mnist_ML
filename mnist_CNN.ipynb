{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "import scipy.signal as sps\n",
    "import string as str\n",
    "\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1\n",
    "\n",
    "def random_name():\n",
    "    return \"\".join(rnd.choices(str.ascii_letters + str.digits, k=8))\n",
    "\n",
    "def argmax2D(matrix):\n",
    "    x, y = 0, 0\n",
    "    for i in range(1, matrix.shape[0]):\n",
    "        for j in range(1, matrix.shape[1]):\n",
    "            if matrix[i, j] > matrix[x, y]:\n",
    "                x, y = i, j\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def leaky_ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0.1 * X, X)\n",
    "\n",
    "def ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :X: Matrix of values.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return X > 0\n",
    "\n",
    "def leaky_ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.where(X > 0, 1, 0.1)\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :X: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    #exp = np.exp(X - np.max(X))\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\") / 255\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\") / 255\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            np.save(f\"{self.name}_K.npy\", self.kernels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(f\"{self.name}_W.npy\", self.weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(f\"{self.name}_B.npy\", self.biases)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def load(self, kernels: bool, weights: bool, biases: bool):\n",
    "        if kernels:\n",
    "            try:\n",
    "                self.kernels = np.load(self.name + \"_K.npy\")\n",
    "            except:\n",
    "                kernels = False\n",
    "        \n",
    "        if weights:\n",
    "            try:\n",
    "                self.weights = np.load(self.name + \"_W.npy\")\n",
    "            except:\n",
    "                weights = False\n",
    "        \n",
    "        if biases:\n",
    "            try:\n",
    "                self.biases = np.load(self.name + \"_B.npy\")\n",
    "            except:\n",
    "                biases = False\n",
    "        \n",
    "        return kernels, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Layer):\n",
    "    def __init__(self, kernel_count, kernel_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        self.kernel_count = kernel_count\n",
    "        self.kernel_shrink = kernel_size - 1\n",
    "        kernels, _, biases = self.load(True, False, True)\n",
    "        if not kernels:\n",
    "            lower, upper = -(1.0 / np.sqrt(kernel_size * kernel_size)), (1.0 / np.sqrt(kernel_size * kernel_size))\n",
    "            self.kernels = lower + np.random.randn(kernel_count, kernel_size, kernel_size) * (upper - lower)\n",
    "        if not biases:\n",
    "            self.biases = np.zeros((kernel_count, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        input_count = input.shape[0]\n",
    "        output = np.zeros((input_count * self.kernel_count, input.shape[1] - self.kernel_shrink, input.shape[2] - self.kernel_shrink))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                output[k + j] = self.activation(sps.correlate2d(input[i], self.kernels[j], \"valid\") + self.biases[j])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        new_dOutput = None\n",
    "        if self.activation_deriv is not None:\n",
    "            new_dOutput = np.zeros_like(self.input)\n",
    "            rotated_kernels = np.rot90(self.kernels, 2, (1, 2))\n",
    "            for i in range(self.input.shape[0]):\n",
    "                k = i * self.kernel_count\n",
    "                for j in range(self.kernel_count):\n",
    "                    new_dOutput[i] += sps.correlate2d(rotated_kernels[j], dOutput[k + j], \"full\")\n",
    "                    \n",
    "            new_dOutput = new_dOutput * self.activation_deriv(self.input)\n",
    "\n",
    "        dKernels = np.zeros_like(self.kernels)\n",
    "        for i in range(sample_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                dKernels[j] += sps.correlate2d(self.input[i], dOutput[k + j], \"valid\")\n",
    "        \n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "        self.kernels = self.kernels - (dKernels / sample_count) * learning_rate\n",
    "\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, window_size, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.input_count = input.shape[0]\n",
    "        self.pooled_width = int(input.shape[1] / self.window_size)\n",
    "        self.pooled_height = int(input.shape[2] / self.window_size)\n",
    "        pooled = np.zeros((self.input_count, self.pooled_width, self.pooled_height))\n",
    "        \n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    pooled[i, j, k] = np.max(input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "        \n",
    "        return pooled\n",
    "\n",
    "    def backward(self, dOutput, *_):\n",
    "        new_dOutput = np.zeros_like(self.input)\n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    x, y = argmax2D(self.input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "                    x += l\n",
    "                    y += m\n",
    "                    new_dOutput[i, x, y] = dOutput[i, j, k]\n",
    "\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(Layer):\n",
    "    def __init__(self, channel_count, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.channel_count = channel_count\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        return np.reshape(input, (-1, input.shape[1] * input.shape[2] * self.channel_count)).T\n",
    "    \n",
    "    def backward(self, dOutput, *_):\n",
    "        return np.reshape(dOutput.T, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        _, weights, biases = self.load(False, True, True)\n",
    "        if not weights:\n",
    "            self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
    "        if not biases:\n",
    "            self.biases = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.weights.dot(input) + self.biases)\n",
    "    \n",
    "    def backward(self, dOutput, sample_count, learning_rate):\n",
    "        new_dOutput = None\n",
    "        if self.activation_deriv is not None:\n",
    "            new_dOutput = self.weights.T.dot(dOutput) * self.activation_deriv(self.input)\n",
    "            \n",
    "        self.weights = self.weights - (dOutput.dot(self.input.T) / sample_count) * learning_rate\n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "        return new_dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_data, training_labels, sample_count, name = random_name(), *layers: Layer):\n",
    "        self.training_data = training_data\n",
    "        self.training_data_count = training_data.shape[0]\n",
    "        self.sample_count = sample_count\n",
    "        self.training_labels = training_labels\n",
    "        self.name = name\n",
    "        self.layers = layers\n",
    "    \n",
    "    def train(self, iterations, learning_rate):\n",
    "        sample_index = self.sample_count\n",
    "        for _ in range(iterations):\n",
    "            input = self.training_data[sample_index - self.sample_count : sample_index]\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "\n",
    "            dOutput = input - self.training_labels[:, sample_index - self.sample_count : sample_index]\n",
    "            for i in range(len(self.layers) - 1, -1, -1):\n",
    "                dOutput = self.layers[i].backward(dOutput, self.sample_count, learning_rate)\n",
    "\n",
    "            sample_index += self.sample_count\n",
    "            if sample_index > self.training_data_count:\n",
    "                sample_index = self.sample_count\n",
    "        \n",
    "    def save(self):\n",
    "        for layer in self.layers:\n",
    "            layer.save()\n",
    "\n",
    "    def assess(self, test_data, test_labels, text):\n",
    "        input = test_data\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax(input, 0) == test_labels) / test_labels.size)\n",
    "        print(f\"\\n############################# Neural Network '{self.name}' Results #############################\")\n",
    "        print(f\"Accuracy {text}:\", accuracy, \"({:.2f}%)\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "test_data, test_labels = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################# Neural Network 'CL_CL_CL_FL_FCL_OL' Results #############################\n",
      "Accuracy on training set: 0.9932333333333333 (99.32%)\n",
      "\n",
      "############################# Neural Network 'CL_CL_CL_FL_FCL_OL' Results #############################\n",
      "Accuracy on test set: 0.9759 (97.59%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "neural_network_1 = NeuralNetwork(training_data, one_hot(training_labels), 1000, \"CL_CL_CL_FL_FCL_OL\",\n",
    "                              ConvolutionLayer(2, 3, leaky_ReLU, None, \"CL_CL_CL_FL_FCL_OL/CL1\"),\n",
    "                              ConvolutionLayer(2, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_OL/CL2\"),\n",
    "                              ConvolutionLayer(2, 5, leaky_ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_OL/CL3\"),\n",
    "                              FlatteningLayer(8, \"CL_CL_CL_FL_FCL_OL/FL1\"), \n",
    "                              FullyConnectedLayer(3200, 80, ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_OL/FCL1\"), \n",
    "                              FullyConnectedLayer(80, 10, softmax, ReLU_deriv, \"CL_CL_CL_FL_FCL_OL/OL\"))\n",
    "neural_network_1.train(1, 0.1)\n",
    "neural_network_1.save()\n",
    "neural_network_1.assess(training_data, training_labels, \"on training set\")\n",
    "neural_network_1.assess(test_data, test_labels, \"on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4389/3754511091.py:54: RuntimeWarning: overflow encountered in exp\n",
      "  exp = np.exp(X)\n",
      "/tmp/ipykernel_4389/3754511091.py:55: RuntimeWarning: invalid value encountered in divide\n",
      "  return exp / np.sum(exp, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m neural_network_2 \u001b[39m=\u001b[39m NeuralNetwork(training_data, one_hot(training_labels), \u001b[39m100\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCL_CL_CL_FL_FCL_FCL_OL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                               ConvolutionLayer(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, leaky_ReLU, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCL_CL_CL_FL_FCL_FCL_OL/CL1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                               ConvolutionLayer(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, leaky_ReLU, leaky_ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mCL_CL_CL_FL_FCL_FCL_OL/CL2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m400\u001b[39m, \u001b[39m80\u001b[39m, ReLU, leaky_ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mCL_CL_CL_FL_FCL_FCL_OL/FCL2\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m80\u001b[39m, \u001b[39m10\u001b[39m, softmax, ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mCL_CL_CL_FL_FCL_FCL_OL/OL\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m neural_network_2\u001b[39m.\u001b[39;49mtrain(\u001b[39m600\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m neural_network_2\u001b[39m.\u001b[39msave()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m neural_network_2\u001b[39m.\u001b[39massess(training_data, training_labels, \u001b[39m\"\u001b[39m\u001b[39mon training set\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 13\u001b[0m in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_data[sample_index \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_count : sample_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m dOutput \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_labels[:, sample_index \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_count : sample_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 13\u001b[0m in \u001b[0;36mConvolutionLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     k \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_count\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_count):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         output[k \u001b[39m+\u001b[39m j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(sps\u001b[39m.\u001b[39;49mcorrelate2d(\u001b[39minput\u001b[39;49m[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernels[j], \u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases[j])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/signal/_signaltools.py:1814\u001b[0m, in \u001b[0;36mcorrelate2d\u001b[0;34m(in1, in2, mode, boundary, fillvalue)\u001b[0m\n\u001b[1;32m   1812\u001b[0m val \u001b[39m=\u001b[39m _valfrommode(mode)\n\u001b[1;32m   1813\u001b[0m bval \u001b[39m=\u001b[39m _bvalfromboundary(boundary)\n\u001b[0;32m-> 1814\u001b[0m out \u001b[39m=\u001b[39m _sigtools\u001b[39m.\u001b[39;49m_convolve2d(in1, in2\u001b[39m.\u001b[39;49mconj(), \u001b[39m0\u001b[39;49m, val, bval, fillvalue)\n\u001b[1;32m   1816\u001b[0m \u001b[39mif\u001b[39;00m swapped_inputs:\n\u001b[1;32m   1817\u001b[0m     out \u001b[39m=\u001b[39m out[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neural_network_2 = NeuralNetwork(training_data, one_hot(training_labels), 100, \"CL_CL_CL_FL_FCL_FCL_OL\",\n",
    "                              ConvolutionLayer(3, 3, leaky_ReLU, None, \"CL_CL_CL_FL_FCL_FCL_OL/CL1\"),\n",
    "                              ConvolutionLayer(3, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_FCL_OL/CL2\"),\n",
    "                              ConvolutionLayer(2, 5, leaky_ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_FCL_OL/CL3\"),\n",
    "                              FlatteningLayer(18, \"CL_CL_CL_FL_FCL_OL/FL1\"), \n",
    "                              FullyConnectedLayer(7200, 400, leaky_ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_FCL_OL/FCL1\"),\n",
    "                              FullyConnectedLayer(400, 80, ReLU, leaky_ReLU_deriv, \"CL_CL_CL_FL_FCL_FCL_OL/FCL2\"), \n",
    "                              FullyConnectedLayer(80, 10, softmax, ReLU_deriv, \"CL_CL_CL_FL_FCL_FCL_OL/OL\"))\n",
    "neural_network_2.train(600, 0.1)\n",
    "neural_network_2.save()\n",
    "neural_network_2.assess(training_data, training_labels, \"on training set\")\n",
    "neural_network_2.assess(test_data, test_labels, \"on test set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
