{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "import scipy.signal as sps\n",
    "import string as str\n",
    "\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1\n",
    "\n",
    "def random_name():\n",
    "    return \"\".join(rnd.choices(str.ascii_letters + str.digits, k=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def leaky_ReLU(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0.1 * X, X)\n",
    "\n",
    "def ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :X: Matrix of values.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return X > 0\n",
    "\n",
    "def leaky_ReLU_deriv(X: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :X: Matrix of values.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.where(X > 0, 1, 0.1)\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :L: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    return np.exp(L) / sum(np.exp(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\") / 255\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\") / 255\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            np.save(self.name + \"_K.npy\", self.kernels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_W.npy\", self.weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_B.npy\", self.biases)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def load(self, kernels: bool, weights: bool, biases: bool):\n",
    "        if kernels:\n",
    "            try:\n",
    "                self.kernels = np.load(self.name + \"_K.npy\")\n",
    "            except:\n",
    "                kernels = False\n",
    "        \n",
    "        if weights:\n",
    "            try:\n",
    "                self.weights = np.load(self.name + \"_W.npy\")\n",
    "            except:\n",
    "                weights = False\n",
    "        \n",
    "        if biases:\n",
    "            try:\n",
    "                self.biases = np.load(self.name + \"_B.npy\")\n",
    "            except:\n",
    "                biases = False\n",
    "        \n",
    "        return kernels, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Layer):\n",
    "    def __init__(self, kernel_count, kernel_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        self.kernel_count = kernel_count\n",
    "        kernels, _, biases = self.load(True, False, True)\n",
    "        if not kernels:\n",
    "            self.kernels = np.random.rand(kernel_count, kernel_size, kernel_size) - 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.zeros(kernel_count)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.kernel_count = self.kernels.shape[0]\n",
    "        input_count = input.shape[0]\n",
    "        output = np.zeros((input_count * self.kernel_count, input.shape[1] - 2, input.shape[2] - 2))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                output[k + j] = self.activation(sps.correlate2d(input[i], self.kernels[j], \"valid\") + self.biases[j])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        dKernels = np.zeros_like(self.kernels)\n",
    "        for i in range(sample_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                dKernels[j] += sps.correlate2d(self.input[i], dOutput[k + j], \"valid\")\n",
    "        \n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "        self.kernels = self.kernels - (dKernels / sample_count) * learning_rate\n",
    "    \n",
    "    def backward(self, dOutput):\n",
    "        if self.activation_deriv is not None:\n",
    "            d_output = np.zeros_like(self.input)\n",
    "            rotated_kernels = np.rot90(self.kernels, 2, (1, 2))\n",
    "            for i in range(self.input.shape[0]):\n",
    "                k = i * self.kernel_count\n",
    "                for j in range(self.kernel_count):\n",
    "                    d_output[i] += sps.correlate2d(rotated_kernels[j], dOutput[k + j], \"full\")\n",
    "                    \n",
    "            return d_output * self.activation_deriv(self.input)\n",
    "        else:\n",
    "            return dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, window_size, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.input_count = input.shape[0]\n",
    "        self.pooled_width = int(input.shape[1] / self.window_size)\n",
    "        self.pooled_height = int(input.shape[2] / self.window_size)\n",
    "        pooled = np.zeros((self.input_count, self.pooled_width, self.pooled_height))\n",
    "        \n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    pooled[i, j, k] = np.max(input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def adjust(self, *_):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        d_output = np.zeros_like(self.input)\n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    indices = np.argmax(self.input[i, l:l+self.window_size, m:m+self.window_size], axis=0, keepdims=True,)[0] + [l, m]\n",
    "                    d_output[i, indices[0], indices[1]] = self.input[i, indices[0], indices[1]] - dOutput[i, j, k]\n",
    "\n",
    "        return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(Layer):\n",
    "    def __init__(self, channel_count, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.channel_count = channel_count\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        return np.reshape(input, (-1, input.shape[1] * input.shape[2] * self.channel_count)).T\n",
    "    \n",
    "    def adjust(self, *_):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dOuput):\n",
    "        return np.reshape(dOuput.T, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        _, weights, biases = self.load(False, True, True)\n",
    "        if not weights:\n",
    "            self.weights = np.random.rand(output_size, input_size) - 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.random.rand(output_size, 1) - 0.5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.weights.dot(input) + self.biases)\n",
    "    \n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        self.weights = self.weights - (dOutput.dot(self.input.T) / sample_count) * learning_rate\n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "    \n",
    "    def backward(self, dOutput):\n",
    "        if self.activation_deriv is not None:\n",
    "            return self.weights.T.dot(dOutput) * self.activation_deriv(self.input)\n",
    "        else:\n",
    "            return dOutput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_data, training_labels, sample_count, *layers: Layer):\n",
    "        self.training_data = training_data\n",
    "        self.sample_count = sample_count\n",
    "        self.training_labels = training_labels\n",
    "        self.one_hot_training_labels = one_hot(training_labels)\n",
    "        #self.one_hot_training_labels = training_labels\n",
    "        self.layers = layers\n",
    "    \n",
    "    def train(self, iterations, learning_rate):\n",
    "        for _ in range(iterations):\n",
    "            input = self.training_data\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "\n",
    "            dOutput = input - self.one_hot_training_labels\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                d_utput = self.layers[i].backward(dOutput)\n",
    "                self.layers[i].adjust(dOutput, self.sample_count, learning_rate)\n",
    "                dOutput = d_utput\n",
    "\n",
    "            self.layers[0].adjust(dOutput, self.sample_count, learning_rate)\n",
    "        \n",
    "    def save(self):\n",
    "        for layer in self.layers:\n",
    "            layer.save()\n",
    "\n",
    "    def assess(self, test_data, test_labels, on_trainig_set = True):\n",
    "        input = test_data\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax(input, 0) == test_labels) / test_labels.size)\n",
    "        print(\"\\n############################# Neural Network Results #############################\\n\")\n",
    "        print(\"Accuracy on test set: \", accuracy)\n",
    "\n",
    "        if on_trainig_set:\n",
    "            input = self.training_data\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "            \n",
    "            accuracy = (np.sum(np.argmax(input, 0) == self.training_labels) / self.training_labels.size)\n",
    "            print(\"Accuracy on training set: \", accuracy)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3913/2308149203.py:53: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(L) / sum(np.exp(L))\n",
      "/tmp/ipykernel_3913/2308149203.py:53: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(L) / sum(np.exp(L))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sample_count \u001b[39m=\u001b[39m training_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m neural_network \u001b[39m=\u001b[39m NeuralNetwork(training_data, training_labels, sample_count, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                               ConvolutionLayer(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, leaky_ReLU, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCL1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                               ConvolutionLayer(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, leaky_ReLU, leaky_ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mCL2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m576\u001b[39m, \u001b[39m56\u001b[39m, ReLU, leaky_ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mFCL1\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m56\u001b[39m, \u001b[39m10\u001b[39m, softmax, ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mOL\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m neural_network\u001b[39m.\u001b[39;49mtrain(\u001b[39m5\u001b[39;49m, \u001b[39m0.15\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m neural_network\u001b[39m.\u001b[39massess(training_data, training_labels, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m dOutput \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_hot_training_labels\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     d_utput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i]\u001b[39m.\u001b[39;49mbackward(dOutput)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39madjust(dOutput, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_count, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     dOutput \u001b[39m=\u001b[39m d_utput\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36mConvolutionLayer.backward\u001b[0;34m(self, dOutput)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         k \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_count\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_count):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             d_output[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sps\u001b[39m.\u001b[39;49mcorrelate2d(rotated_kernels[j], dOutput[k \u001b[39m+\u001b[39;49m j], \u001b[39m\"\u001b[39;49m\u001b[39mfull\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m d_output \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_deriv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/signal/_signaltools.py:1814\u001b[0m, in \u001b[0;36mcorrelate2d\u001b[0;34m(in1, in2, mode, boundary, fillvalue)\u001b[0m\n\u001b[1;32m   1812\u001b[0m val \u001b[39m=\u001b[39m _valfrommode(mode)\n\u001b[1;32m   1813\u001b[0m bval \u001b[39m=\u001b[39m _bvalfromboundary(boundary)\n\u001b[0;32m-> 1814\u001b[0m out \u001b[39m=\u001b[39m _sigtools\u001b[39m.\u001b[39;49m_convolve2d(in1, in2\u001b[39m.\u001b[39;49mconj(), \u001b[39m0\u001b[39;49m, val, bval, fillvalue)\n\u001b[1;32m   1816\u001b[0m \u001b[39mif\u001b[39;00m swapped_inputs:\n\u001b[1;32m   1817\u001b[0m     out \u001b[39m=\u001b[39m out[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "training_labels = training_labels[0:10000]\n",
    "training_data = training_data[0:10000]\n",
    "sample_count = training_data.shape[0]\n",
    "\n",
    "neural_network = NeuralNetwork(training_data, training_labels, sample_count, \n",
    "                              ConvolutionLayer(2, 3, leaky_ReLU, None, \"CL1\"),\n",
    "                              ConvolutionLayer(2, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL2\"),\n",
    "                              MaxPoolLayer(2, \"MPL1\"),\n",
    "                              FlatteningLayer(4, \"FL1\"), \n",
    "                              FullyConnectedLayer(576, 56, ReLU, leaky_ReLU_deriv, \"FCL1\"), \n",
    "                              FullyConnectedLayer(56, 10, softmax, ReLU_deriv, \"OL\"))\n",
    "neural_network.train(5, 0.15)\n",
    "neural_network.assess(training_data, training_labels, False)\n",
    "\n",
    "#conv_layer1 = ConvolutionLayer(2, 3, ReLU)\n",
    "#conv_layer2 = ConvolutionLayer(2, 3, ReLU)\n",
    "#max_pool_layer = MaxPoolLayer(2)\n",
    "#flat_layer = FlatteningLayer(2, 4)\n",
    "#\n",
    "#output = conv_layer1.forward(training_data[0:2])\n",
    "#output = conv_layer2.forward(output)\n",
    "#output = max_pool_layer.forward(output)\n",
    "#flat = flat_layer.forward(output)\n",
    "#\n",
    "#for i in output:\n",
    "#    plt.imshow(i, cmap='gray')\n",
    "#    plt.show()\n",
    "#\n",
    "#print(flat.shape)\n",
    "#plt.imshow(np.reshape(flat[0:144, 0], (12, 12)), cmap='gray')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03488215 -0.35531922  0.49783431]\n",
      " [ 0.17666894  0.2640178  -0.49180329]\n",
      " [-0.39097804  0.36370227 -0.30382147]] 0.18349465609423865 \n",
      " [[ 0.05471702 -0.2576588   0.36614072]\n",
      " [ 0.41563976  0.05004127 -0.31066925]\n",
      " [-0.23599637 -0.19855679  0.49957401]] -0.4685399982521068\n",
      "[[ 0.27294496  0.04879508  0.16855355  0.41460589]\n",
      " [ 0.          0.5190993   0.08130854  0.21250987]\n",
      " [ 0.42098703  0.10662714  0.09466064  0.27126232]\n",
      " [-0.00390969  0.10055107  0.4918398   0.08297553]]\n",
      "[[0.6 0.4]\n",
      " [0.9 0.3]] 0.18349465609423865 0.7306791494348426\n"
     ]
    }
   ],
   "source": [
    "l = np.array([[ 6, 4],\n",
    "              [ 9, 3]]) / 10\n",
    "m = np.array([[1, 2, 0, 1, 1, 1], \n",
    "              [2, 1, 1, 1, 2, 0],\n",
    "              [1, 1, 2, 0, 0, 1],\n",
    "              [2, 2, 0, 1, 1, 2],\n",
    "              [1, 0, 1, 2, 0, 1],\n",
    "              [0, 2, 1, 0, 1, 2]]) / 10\n",
    "k1 = np.array([[ 1,  0, -1], \n",
    "               [ 0,  1, -1],\n",
    "               [-1, -1,  1]]) / 10\n",
    "k2 = np.array([[ 0, -1,  1], \n",
    "               [ 1,  0, -1],\n",
    "               [ 0, -1,  1]]) / 10\n",
    "k1 = np.random.rand(3, 3) - 0.5\n",
    "k2 = np.random.rand(3, 3) - 0.5\n",
    "b1 = np.random.random() - 0.5\n",
    "b2 = np.random.random() - 0.5\n",
    "kt1 = np.array([[ 0,  1,  0], \n",
    "                [-1,  0,  1],\n",
    "                [ 1,  1,  0]]) / 10\n",
    "kt2 = np.array([[ 1,  1,  1], \n",
    "                [-1,  0, -1],\n",
    "                [ 0,  1,  0]]) / 10\n",
    "print(k1, b1, \"\\n\", k2, b2)\n",
    "\n",
    "for _ in range(2000):\n",
    "    y1 = sps.correlate2d(m, k1, \"valid\")\n",
    "    ry1 = leaky_ReLU(y1 + b1)\n",
    "    \n",
    "    y2 = sps.correlate2d(ry1, k2, \"valid\")\n",
    "    ry2 = leaky_ReLU(y2 + b2)\n",
    "\n",
    "    d1 = ry2 - l\n",
    "\n",
    "    d2 = sps.correlate2d(np.rot90(k2, 2), d1, \"full\") * leaky_ReLU_deriv(ry1)\n",
    "\n",
    "    dk2 = sps.correlate2d(ry1, d1, \"valid\")\n",
    "    k2 = k2 - dk2 * 0.15\n",
    "    b2 = b2 - np.sum(d1) * 0.15\n",
    "\n",
    "    dk1 = sps.correlate2d(m, d2, \"valid\")\n",
    "    k1 = k1 - dk1 * 0.15\n",
    "    b2 = b2 - np.sum(d2) * 0.15\n",
    "\n",
    "y1 = sps.correlate2d(m, k1, \"valid\")\n",
    "ry1 = leaky_ReLU(y1 + b1)\n",
    "print(ry1)\n",
    "y2 = sps.correlate2d(ry1, k2, \"valid\")\n",
    "ry2 = leaky_ReLU(y2 + b2)\n",
    "print(ry2, b1, b2)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.6 0.4]\n",
      "  [0.9 0.3]]\n",
      "\n",
      " [[0.5 0.3]\n",
      "  [0.1 0.2]]]\n",
      "[[[0.60177609 0.39801728]\n",
      "  [0.89847964 0.30219653]]\n",
      "\n",
      " [[0.49627562 0.30000562]\n",
      "  [0.1044547  0.19852644]]]\n"
     ]
    }
   ],
   "source": [
    "l = np.array([[[ 6, 4], [ 9, 3]], [[ 5, 3], [ 1, 2]]]) / 10\n",
    "print(l)\n",
    "m = np.array([[[1, 2, 0, 1, 1, 1], \n",
    "               [2, 1, 1, 1, 2, 0],\n",
    "               [1, 1, 2, 0, 0, 1],\n",
    "               [2, 2, 0, 1, 1, 2],\n",
    "               [1, 0, 1, 2, 0, 1],\n",
    "               [0, 2, 1, 0, 1, 2]],\n",
    "               [[0, 1, 2, 1, 1, 1], \n",
    "               [2, 1, 3, 1, 0, 0],\n",
    "               [1, 1, 2, 0, 0, 1],\n",
    "               [0, 2, 1, 1, 1, 2],\n",
    "               [1, 0, 1, 1, 0, 1],\n",
    "               [0, 1, 1, 0, 1, 2]],]) / 10\n",
    "k1 = np.array([[ 1,  0, -1], \n",
    "               [ 0,  1, -1],\n",
    "               [-1, -1,  1]]) / 10\n",
    "k2 = np.array([[ 0, -1,  1], \n",
    "               [ 1,  0, -1],\n",
    "               [ 0, -1,  1]]) / 10\n",
    "\n",
    "neural_network = NeuralNetwork(m, l, 2, \n",
    "                              ConvolutionLayer(1, 3, leaky_ReLU, None, \"CL1\"),\n",
    "                              ConvolutionLayer(1, 3, leaky_ReLU, leaky_ReLU_deriv, \"CL2\"))\n",
    "neural_network.train(2500, 0.15)\n",
    "neural_network.assess(m, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1. , 0.1, 0.1])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLU(np.array([1, 2, -2, -1]))\n",
    "leaky_ReLU_deriv(leaky_ReLU(np.array([1, 2, -2, -1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
