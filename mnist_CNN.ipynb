{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import random as rnd\n",
    "import numpy.typing as npt\n",
    "import scipy.signal as sps\n",
    "import string as str\n",
    "\n",
    "IMAGE_EDGE_SIZE = 28\n",
    "PIXELS_PER_IMAGE = IMAGE_EDGE_SIZE ** 2\n",
    "CLASSES_COUNT = 10\n",
    "ITERATIONS = 500\n",
    "LEARNING_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: npt.ArrayLike) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts a 1D array of labels (the ground truth) to 2D matrix of shape (10, labels.size) as a probability distribution, \n",
    "    where the corresponding row given by the label value has probability of 1.\n",
    "    \n",
    "    :labels: The ground truth.\n",
    "    :return: Encoded values of labels to probability distribution.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((10, labels.size))\n",
    "    one_hot[labels, np.arange(labels.size)] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_accuracy(results: npt.NDArray, labels: npt.ArrayLike) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a neural network from the results of classification by comparing it to the ground truth.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :return: The accuracy as a real number. \n",
    "    \"\"\"\n",
    "    return (np.sum(np.argmax(results, 0) == labels) / labels.size)\n",
    "\n",
    "def show_some_mistakes(results: npt.NDArray, labels: npt.ArrayLike, data: npt.NDArray, samples = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots randomly choosen images, which were not classified correctly.\n",
    "\n",
    "    :results: The forward propagation results.\n",
    "    :labels: The ground truth.\n",
    "    :data: The input data of forward propagation, i.e images.\n",
    "    :samples: The number of shown images, 10 by default.\n",
    "    \"\"\"\n",
    "    results = np.argmax(results, 0)\n",
    "    i = rnd.randint(0, labels.size)\n",
    "    j = 0\n",
    "    while j < samples:\n",
    "        i = (i + 1) % labels.size\n",
    "        if results[i] != labels[i]:\n",
    "            print(\"labeled:\", labels[i], \"-- classified:\", results[i])\n",
    "            plt.imshow(data[:, i].reshape((IMAGE_EDGE_SIZE, IMAGE_EDGE_SIZE)), cmap='gray')\n",
    "            plt.show()\n",
    "            j += 1\n",
    "\n",
    "def random_name():\n",
    "    return \"\".join(rnd.choices(str.ascii_letters + str.digits, k=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Rectified Linear Units of a numpy matrix.\n",
    "    \n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all nonnegative numbers returns its value, otherwise 0.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, L)\n",
    "\n",
    "def ReLU_deriv(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the derivation of ReLu function of a numpy matrix.\n",
    "\n",
    "    :L: Matrix of values of a hidden layer.\n",
    "    :return: For all positive numbers returns 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    return L > 0\n",
    "\n",
    "def sigmoid(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Calculates the Sigmoid function of a numpy matrix.\n",
    "    \n",
    "    :L: Values of a hidden layer.\n",
    "    :return: For all indexes with value x returns 1 / (1 + e^(-x)).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-L))\n",
    "\n",
    "def softmax(L: npt.NDArray) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Converts matrix of N values in a row to probability distribution of N outcomes for each row.\n",
    "\n",
    "    :L: Values of an output layer.\n",
    "    :return: For all indexes of the given matrix returns the probability of a given index in its row.\n",
    "    \"\"\"\n",
    "    return np.exp(L) / sum(np.exp(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads training data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of training data and array of training labels.\n",
    "    \"\"\"\n",
    "    training_data = idx2numpy.convert_from_file(\"mnist/train-images.idx3-ubyte\") / 255\n",
    "    training_labels = idx2numpy.convert_from_file(\"mnist/train-labels.idx1-ubyte\")\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_test_data() -> tuple:\n",
    "    \"\"\"\n",
    "    Loads testing data and training labels from files and transforms them to desired shape.\n",
    "\n",
    "    :return: Matrix of testing data and array of testing labels.\n",
    "    \"\"\"\n",
    "    test_data = idx2numpy.convert_from_file(\"mnist/t10k-images.idx3-ubyte\") / 255\n",
    "    test_labels = idx2numpy.convert_from_file(\"mnist/t10k-labels.idx1-ubyte\")\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            np.save(self.name + \"_K.npy\", self.kernels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_W.npy\", self.weights)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        try:\n",
    "            np.save(self.name + \"_B.npy\", self.biases)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def load(self, kernels: bool, weights: bool, biases: bool):\n",
    "        if kernels:\n",
    "            try:\n",
    "                self.kernels = np.load(self.name + \"_K.npy\")\n",
    "            except:\n",
    "                kernels = False\n",
    "        \n",
    "        if weights:\n",
    "            try:\n",
    "                self.weights = np.load(self.name + \"_W.npy\")\n",
    "            except:\n",
    "                weights = False\n",
    "        \n",
    "        if biases:\n",
    "            try:\n",
    "                self.biases = np.load(self.name + \"_B.npy\")\n",
    "            except:\n",
    "                biases = False\n",
    "        \n",
    "        return kernels, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer(Layer):\n",
    "    def __init__(self, kernel_count, kernel_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        self.kernel_count = kernel_count\n",
    "        kernels, _, biases = self.load(True, False, True)\n",
    "        if not kernels:\n",
    "            self.kernels = np.random.rand(kernel_count, kernel_size, kernel_size) * 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.random.rand(kernel_count) * 0.5\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.kernel_count = self.kernels.shape[0]\n",
    "        input_count = input.shape[0]\n",
    "        output = np.zeros((input_count * self.kernel_count, input.shape[1] - 2, input.shape[2] - 2))\n",
    "        \n",
    "        for i in range(input_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                output[k + j] = self.activation(sps.correlate2d(input[i], self.kernels[j], \"valid\") + self.biases[j])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        dKernels = np.zeros_like(self.kernels)\n",
    "        for i in range(sample_count):\n",
    "            k = i * self.kernel_count\n",
    "            for j in range(self.kernel_count):\n",
    "                dKernels[j] += sps.correlate2d(self.input[i], dOutput[k + j], \"valid\")\n",
    "\n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate * 0.1\n",
    "        print(self.biases)\n",
    "        self.kernels = self.kernels - (dKernels / sample_count) * learning_rate\n",
    "    \n",
    "    def backward(self, dOutput):\n",
    "        if self.activation_deriv is not None:\n",
    "            d_output = np.zeros_like(self.input)\n",
    "            rotated_kernels = np.rot90(self.kernels, 2, (1, 2))\n",
    "            for i in range(self.input.shape[0]):\n",
    "                k = i * self.kernel_count\n",
    "                for j in range(self.kernel_count):\n",
    "                    d_output[i] += sps.correlate2d(rotated_kernels[j], dOutput[k + j], \"full\")\n",
    "                    \n",
    "            return d_output * self.activation_deriv(self.input)\n",
    "        else:\n",
    "            return dOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, window_size, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.input_count = input.shape[0]\n",
    "        self.pooled_width = int(input.shape[1] / self.window_size)\n",
    "        self.pooled_height = int(input.shape[2] / self.window_size)\n",
    "        pooled = np.zeros((self.input_count, self.pooled_width, self.pooled_height))\n",
    "        \n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    pooled[i, j, k] = np.max(input[i, l:l+self.window_size, m:m+self.window_size])\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def adjust(self, *_):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dOutput):\n",
    "        d_output = np.zeros_like(self.input)\n",
    "        for i in range(self.input_count):\n",
    "            for j in range(self.pooled_width):\n",
    "                for k in range(self.pooled_height):\n",
    "                    l = j * self.window_size\n",
    "                    m = k * self.window_size\n",
    "                    indices = np.argmax(self.input[i, l:l+self.window_size, m:m+self.window_size], axis=0, keepdims=True,)[0] + [l, m]\n",
    "                    d_output[i, indices[0], indices[1]] = self.input[i, indices[0], indices[1]] - dOutput[i, j, k]\n",
    "\n",
    "        return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatteningLayer(Layer):\n",
    "    def __init__(self, channel_count, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.channel_count = channel_count\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        return np.reshape(input, (-1, input.shape[1] * input.shape[2] * self.channel_count)).T\n",
    "    \n",
    "    def adjust(self, *_):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dOuput):\n",
    "        return np.reshape(dOuput.T, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, activation, activation_deriv, name = random_name()):\n",
    "        Layer.__init__(self, name)\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "        _, weights, biases = self.load(False, True, True)\n",
    "        if not weights:\n",
    "            self.weights = np.random.rand(output_size, input_size) - 0.5\n",
    "        if not biases:\n",
    "            self.biases = np.random.rand(output_size, 1) - 0.5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.weights.dot(input) + self.biases)\n",
    "    \n",
    "    def adjust(self, dOutput, sample_count, learning_rate):\n",
    "        self.weights = self.weights - (dOutput.dot(self.input.T) / sample_count) * learning_rate\n",
    "        self.biases = self.biases - (np.sum(dOutput) / sample_count) * learning_rate\n",
    "    \n",
    "    def backward(self, dOutput):\n",
    "        if self.activation_deriv is not None:\n",
    "            return self.weights.T.dot(dOutput) * self.activation_deriv(self.input)\n",
    "        else:\n",
    "            return dOutput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_data, training_labels, sample_count, *layers: Layer):\n",
    "        self.training_data = training_data\n",
    "        self.sample_count = sample_count\n",
    "        self.training_labels = training_labels\n",
    "        #self.one_hot_training_labels = one_hot(training_labels)\n",
    "        self.one_hot_training_labels = training_labels\n",
    "        self.layers = layers\n",
    "    \n",
    "    def train(self, iterations, learning_rate):\n",
    "        for _ in range(iterations):\n",
    "            input = self.training_data\n",
    "            for layer in self.layers:\n",
    "                input = layer.forward(input)\n",
    "\n",
    "            dOutput = input - self.one_hot_training_labels\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                d_utput = self.layers[i].backward(dOutput)\n",
    "                self.layers[i].adjust(dOutput, self.sample_count, learning_rate)\n",
    "                dOutput = d_utput\n",
    "\n",
    "            self.layers[0].adjust(dOutput, self.sample_count, learning_rate)\n",
    "        \n",
    "    def save(self):\n",
    "        for layer in self.layers:\n",
    "            layer.save()\n",
    "\n",
    "    def assess(self, test_data, test_labels, on_trainig_set = True):\n",
    "        input = test_data\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        \n",
    "        print(input)\n",
    "\n",
    "        #accuracy = (np.sum(np.argmax(input, 0) == test_labels) / test_labels.size)\n",
    "        #print(\"\\n############################# Neural Network Results #############################\\n\")\n",
    "        #print(\"Accuracy on test set: \", accuracy)\n",
    "#\n",
    "        #if on_trainig_set:\n",
    "        #    input = self.training_data\n",
    "        #    for layer in self.layers:\n",
    "        #        input = layer.forward(input)\n",
    "        #    \n",
    "        #    accuracy = (np.sum(np.argmax(input, 0) == self.training_labels) / self.training_labels.size)\n",
    "        #    print(\"Accuracy on training set: \", accuracy)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sample_count \u001b[39m=\u001b[39m training_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m neural_network \u001b[39m=\u001b[39m NeuralNetwork(training_data, training_labels, sample_count, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                               ConvolutionLayer(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, ReLU, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCL1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                               ConvolutionLayer(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, ReLU, ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mCL2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m576\u001b[39m, \u001b[39m56\u001b[39m, ReLU, ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mFCL1\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                               FullyConnectedLayer(\u001b[39m56\u001b[39m, \u001b[39m10\u001b[39m, softmax, ReLU_deriv, \u001b[39m\"\u001b[39m\u001b[39mOL\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m neural_network\u001b[39m.\u001b[39;49mtrain(\u001b[39m5\u001b[39;49m, \u001b[39m0.15\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m neural_network\u001b[39m.\u001b[39massess(training_data, training_labels, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m dOutput \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_hot_training_labels\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;32m/home/david/projs/mnist_ML/mnist_CNN.ipynb Cell 11\u001b[0m in \u001b[0;36mMaxPoolLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             l \u001b[39m=\u001b[39m j \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             m \u001b[39m=\u001b[39m k \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             pooled[i, j, k] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmax(\u001b[39minput\u001b[39;49m[i, l:l\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_size, m:m\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_size])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/david/projs/mnist_ML/mnist_CNN.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pooled\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, training_labels = load_training_data()\n",
    "training_labels = training_labels[0:10000]\n",
    "training_data = training_data[0:10000]\n",
    "sample_count = training_data.shape[0]\n",
    "\n",
    "neural_network = NeuralNetwork(training_data, training_labels, sample_count, \n",
    "                              ConvolutionLayer(2, 3, ReLU, None, \"CL1\"),\n",
    "                              ConvolutionLayer(2, 3, ReLU, ReLU_deriv, \"CL2\"),\n",
    "                              MaxPoolLayer(2, \"MPL1\"),\n",
    "                              FlatteningLayer(4, \"FL1\"), \n",
    "                              FullyConnectedLayer(576, 56, ReLU, ReLU_deriv, \"FCL1\"), \n",
    "                              FullyConnectedLayer(56, 10, softmax, ReLU_deriv, \"OL\"))\n",
    "neural_network.train(5, 0.15)\n",
    "neural_network.assess(training_data, training_labels, False)\n",
    "\n",
    "#conv_layer1 = ConvolutionLayer(2, 3, ReLU)\n",
    "#conv_layer2 = ConvolutionLayer(2, 3, ReLU)\n",
    "#max_pool_layer = MaxPoolLayer(2)\n",
    "#flat_layer = FlatteningLayer(2, 4)\n",
    "#\n",
    "#output = conv_layer1.forward(training_data[0:2])\n",
    "#output = conv_layer2.forward(output)\n",
    "#output = max_pool_layer.forward(output)\n",
    "#flat = flat_layer.forward(output)\n",
    "#\n",
    "#for i in output:\n",
    "#    plt.imshow(i, cmap='gray')\n",
    "#    plt.show()\n",
    "#\n",
    "#print(flat.shape)\n",
    "#plt.imshow(np.reshape(flat[0:144, 0], (12, 12)), cmap='gray')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.006 0.004]\n",
      " [0.009 0.003]]\n",
      "[[0.94770706 0.80952027]\n",
      " [0.90461407 0.81922787]]\n",
      "[[0.59991467 0.40012314]\n",
      " [0.90004392 0.29992594]]\n"
     ]
    }
   ],
   "source": [
    "l = np.array([[ 6, 4],\n",
    "              [ 9, 3]]) / 10\n",
    "m = np.array([[1, 2, 0, 1, 1, 1], \n",
    "              [2, 1, 1, 1, 2, 0],\n",
    "              [1, 1, 2, 0, 0, 1],\n",
    "              [2, 2, 0, 1, 1, 2],\n",
    "              [1, 0, 1, 2, 0, 1],\n",
    "              [0, 2, 1, 0, 1, 2]]) / 10\n",
    "k1 = np.array([[ 1,  0, -1], \n",
    "               [ 0,  1, -1],\n",
    "               [-1, -1,  1]]) / 10\n",
    "k2 = np.array([[ 0, -1,  1], \n",
    "               [ 1,  0, -1],\n",
    "               [ 0, -1,  1]]) / 10\n",
    "k1 = np.random.rand(3, 3) * 0.5\n",
    "k2 = np.random.rand(3, 3) * 0.5\n",
    "kt1 = np.array([[ 0,  1,  0], \n",
    "                [-1,  0,  1],\n",
    "                [ 1,  1,  0]]) / 10\n",
    "kt2 = np.array([[ 1,  1,  1], \n",
    "                [-1,  0, -1],\n",
    "                [ 0,  1,  0]]) / 10\n",
    "y = sps.correlate2d(m, kt1, \"valid\")\n",
    "y = sps.correlate2d(y, kt2, \"valid\")\n",
    "print(y)\n",
    "\n",
    "y1 = sps.correlate2d(m, k1, \"valid\")\n",
    "ry1 = ReLU(y1)\n",
    "y2 = sps.correlate2d(ry1, k2, \"valid\")\n",
    "ry2 = ReLU(y2)\n",
    "print(ry2)\n",
    "\n",
    "for _ in range(1000):\n",
    "    y1 = sps.correlate2d(m, k1, \"valid\")\n",
    "    ry1 = ReLU(y1)\n",
    "    \n",
    "    y2 = sps.correlate2d(ry1, k2, \"valid\")\n",
    "    ry2 = ReLU(y2)\n",
    "\n",
    "    d1 = ry2 - l\n",
    "\n",
    "    d2 = sps.correlate2d(np.rot90(k2, 2), d1, \"full\") * ReLU_deriv(ry1)\n",
    "\n",
    "    dk2 = sps.correlate2d(ry1, d1, \"valid\")\n",
    "    k2 = k2 - dk2 * 0.15\n",
    "\n",
    "    dk1 = sps.correlate2d(m, d2, \"valid\")\n",
    "    k1 = k1 - dk1 * 0.15\n",
    "\n",
    "y1 = sps.correlate2d(m, k1, \"valid\")\n",
    "ry1 = ReLU(y1)\n",
    "y2 = sps.correlate2d(ry1, k2, \"valid\")\n",
    "ry2 = ReLU(y2)\n",
    "print(ry2)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36009574]\n",
      "[0.06348167]\n",
      "[0.39166071]\n",
      "[-0.0291623]\n",
      "[0.40116107]\n",
      "[-0.0291623]\n",
      "[0.4100914]\n",
      "[-0.0291623]\n",
      "[0.41848592]\n",
      "[-0.0291623]\n",
      "[0.42637676]\n",
      "[-0.0291623]\n",
      "[0.43379416]\n",
      "[-0.0291623]\n",
      "[0.44076651]\n",
      "[-0.0291623]\n",
      "[0.44732052]\n",
      "[-0.0291623]\n",
      "[0.45348129]\n",
      "[-0.0291623]\n",
      "[0.45927241]\n",
      "[-0.0291623]\n",
      "[0.46471606]\n",
      "[-0.0291623]\n",
      "[0.4698331]\n",
      "[-0.0291623]\n",
      "[0.47464311]\n",
      "[-0.0291623]\n",
      "[0.47916453]\n",
      "[-0.0291623]\n",
      "[0.48341466]\n",
      "[-0.0291623]\n",
      "[0.48740978]\n",
      "[-0.0291623]\n",
      "[0.49116519]\n",
      "[-0.0291623]\n",
      "[0.49469528]\n",
      "[-0.0291623]\n",
      "[0.49801356]\n",
      "[-0.0291623]\n",
      "[0.50113275]\n",
      "[-0.0291623]\n",
      "[0.50406478]\n",
      "[-0.0291623]\n",
      "[0.5068209]\n",
      "[-0.0291623]\n",
      "[0.50941164]\n",
      "[-0.0291623]\n",
      "[0.51184694]\n",
      "[-0.0291623]\n",
      "[0.51413613]\n",
      "[-0.0291623]\n",
      "[0.51628796]\n",
      "[-0.0291623]\n",
      "[0.51831068]\n",
      "[-0.0291623]\n",
      "[0.52021204]\n",
      "[-0.0291623]\n",
      "[0.52199932]\n",
      "[-0.0291623]\n",
      "[0.52367936]\n",
      "[-0.0291623]\n",
      "[0.5252586]\n",
      "[-0.0291623]\n",
      "[0.52674308]\n",
      "[-0.0291623]\n",
      "[0.5281385]\n",
      "[-0.0291623]\n",
      "[0.52945019]\n",
      "[-0.0291623]\n",
      "[0.53068318]\n",
      "[-0.0291623]\n",
      "[0.53184219]\n",
      "[-0.0291623]\n",
      "[0.53293165]\n",
      "[-0.0291623]\n",
      "[0.53395576]\n",
      "[-0.0291623]\n",
      "[0.53491841]\n",
      "[-0.0291623]\n",
      "[0.53582331]\n",
      "[-0.0291623]\n",
      "[0.53667391]\n",
      "[-0.0291623]\n",
      "[0.53747347]\n",
      "[-0.0291623]\n",
      "[0.53822506]\n",
      "[-0.0291623]\n",
      "[0.53893156]\n",
      "[-0.0291623]\n",
      "[0.53959567]\n",
      "[-0.0291623]\n",
      "[0.54021993]\n",
      "[-0.0291623]\n",
      "[0.54080673]\n",
      "[-0.0291623]\n",
      "[0.54135833]\n",
      "[-0.0291623]\n",
      "[0.54187683]\n",
      "[-0.0291623]\n",
      "[0.54236422]\n",
      "[-0.0291623]\n",
      "[0.54282236]\n",
      "[-0.0291623]\n",
      "[0.54325302]\n",
      "[-0.0291623]\n",
      "[0.54365784]\n",
      "[-0.0291623]\n",
      "[0.54403837]\n",
      "[-0.0291623]\n",
      "[0.54439607]\n",
      "[-0.0291623]\n",
      "[0.5447323]\n",
      "[-0.0291623]\n",
      "[0.54504837]\n",
      "[-0.0291623]\n",
      "[0.54534546]\n",
      "[-0.0291623]\n",
      "[0.54562474]\n",
      "[-0.0291623]\n",
      "[0.54588725]\n",
      "[-0.0291623]\n",
      "[0.54613402]\n",
      "[-0.0291623]\n",
      "[0.54636598]\n",
      "[-0.0291623]\n",
      "[0.54658402]\n",
      "[-0.0291623]\n",
      "[0.54678898]\n",
      "[-0.0291623]\n",
      "[0.54698164]\n",
      "[-0.0291623]\n",
      "[0.54716274]\n",
      "[-0.0291623]\n",
      "[0.54733298]\n",
      "[-0.0291623]\n",
      "[0.547493]\n",
      "[-0.0291623]\n",
      "[0.54764342]\n",
      "[-0.0291623]\n",
      "[0.54778481]\n",
      "[-0.0291623]\n",
      "[0.54791772]\n",
      "[-0.0291623]\n",
      "[0.54804266]\n",
      "[-0.0291623]\n",
      "[0.5481601]\n",
      "[-0.0291623]\n",
      "[0.54827049]\n",
      "[-0.0291623]\n",
      "[0.54837426]\n",
      "[-0.0291623]\n",
      "[0.54847181]\n",
      "[-0.0291623]\n",
      "[0.5485635]\n",
      "[-0.0291623]\n",
      "[0.54864969]\n",
      "[-0.0291623]\n",
      "[0.54873071]\n",
      "[-0.0291623]\n",
      "[0.54880687]\n",
      "[-0.0291623]\n",
      "[0.54887845]\n",
      "[-0.0291623]\n",
      "[0.54894575]\n",
      "[-0.0291623]\n",
      "[0.549009]\n",
      "[-0.0291623]\n",
      "[0.54906846]\n",
      "[-0.0291623]\n",
      "[0.54912435]\n",
      "[-0.0291623]\n",
      "[0.54917689]\n",
      "[-0.0291623]\n",
      "[0.54922628]\n",
      "[-0.0291623]\n",
      "[0.5492727]\n",
      "[-0.0291623]\n",
      "[0.54931634]\n",
      "[-0.0291623]\n",
      "[0.54935736]\n",
      "[-0.0291623]\n",
      "[0.54939592]\n",
      "[-0.0291623]\n",
      "[0.54943216]\n",
      "[-0.0291623]\n",
      "[0.54946623]\n",
      "[-0.0291623]\n",
      "[0.54949826]\n",
      "[-0.0291623]\n",
      "[0.54952836]\n",
      "[-0.0291623]\n",
      "[0.54955666]\n",
      "[-0.0291623]\n",
      "[0.54958326]\n",
      "[-0.0291623]\n",
      "[0.54960827]\n",
      "[-0.0291623]\n",
      "[0.54963177]\n",
      "[-0.0291623]\n",
      "[0.54965386]\n",
      "[-0.0291623]\n",
      "[0.54967463]\n",
      "[-0.0291623]\n",
      "[0.54969415]\n",
      "[-0.0291623]\n",
      "[0.54971251]\n",
      "[-0.0291623]\n",
      "[0.54972976]\n",
      "[-0.0291623]\n",
      "[0.54974597]\n",
      "[-0.0291623]\n",
      "[0.54976121]\n",
      "[-0.0291623]\n",
      "[0.54977554]\n",
      "[-0.0291623]\n",
      "[0.54978901]\n",
      "[-0.0291623]\n",
      "[0.54980167]\n",
      "[-0.0291623]\n",
      "[0.54981357]\n",
      "[-0.0291623]\n",
      "[0.54982475]\n",
      "[-0.0291623]\n",
      "[0.54983527]\n",
      "[-0.0291623]\n",
      "[0.54984515]\n",
      "[-0.0291623]\n",
      "[0.54985444]\n",
      "[-0.0291623]\n",
      "[0.54986318]\n",
      "[-0.0291623]\n",
      "[0.54987138]\n",
      "[-0.0291623]\n",
      "[0.5498791]\n",
      "[-0.0291623]\n",
      "[0.54988636]\n",
      "[-0.0291623]\n",
      "[0.54989317]\n",
      "[-0.0291623]\n",
      "[0.54989958]\n",
      "[-0.0291623]\n",
      "[0.54990561]\n",
      "[-0.0291623]\n",
      "[0.54991127]\n",
      "[-0.0291623]\n",
      "[0.5499166]\n",
      "[-0.0291623]\n",
      "[0.5499216]\n",
      "[-0.0291623]\n",
      "[0.5499263]\n",
      "[-0.0291623]\n",
      "[0.54993073]\n",
      "[-0.0291623]\n",
      "[0.54993488]\n",
      "[-0.0291623]\n",
      "[0.54993879]\n",
      "[-0.0291623]\n",
      "[0.54994246]\n",
      "[-0.0291623]\n",
      "[0.54994591]\n",
      "[-0.0291623]\n",
      "[0.54994916]\n",
      "[-0.0291623]\n",
      "[0.54995221]\n",
      "[-0.0291623]\n",
      "[0.54995508]\n",
      "[-0.0291623]\n",
      "[0.54995777]\n",
      "[-0.0291623]\n",
      "[0.54996031]\n",
      "[-0.0291623]\n",
      "[0.54996269]\n",
      "[-0.0291623]\n",
      "[0.54996493]\n",
      "[-0.0291623]\n",
      "[0.54996703]\n",
      "[-0.0291623]\n",
      "[0.54996901]\n",
      "[-0.0291623]\n",
      "[0.54997087]\n",
      "[-0.0291623]\n",
      "[0.54997262]\n",
      "[-0.0291623]\n",
      "[0.54997426]\n",
      "[-0.0291623]\n",
      "[0.5499758]\n",
      "[-0.0291623]\n",
      "[0.54997726]\n",
      "[-0.0291623]\n",
      "[0.54997862]\n",
      "[-0.0291623]\n",
      "[0.5499799]\n",
      "[-0.0291623]\n",
      "[0.54998111]\n",
      "[-0.0291623]\n",
      "[0.54998224]\n",
      "[-0.0291623]\n",
      "[0.54998331]\n",
      "[-0.0291623]\n",
      "[0.54998431]\n",
      "[-0.0291623]\n",
      "[0.54998525]\n",
      "[-0.0291623]\n",
      "[0.54998614]\n",
      "[-0.0291623]\n",
      "[0.54998697]\n",
      "[-0.0291623]\n",
      "[0.54998775]\n",
      "[-0.0291623]\n",
      "[0.54998848]\n",
      "[-0.0291623]\n",
      "[0.54998918]\n",
      "[-0.0291623]\n",
      "[0.54998983]\n",
      "[-0.0291623]\n",
      "[0.54999044]\n",
      "[-0.0291623]\n",
      "[0.54999101]\n",
      "[-0.0291623]\n",
      "[0.54999155]\n",
      "[-0.0291623]\n",
      "[0.54999206]\n",
      "[-0.0291623]\n",
      "[0.54999253]\n",
      "[-0.0291623]\n",
      "[0.54999298]\n",
      "[-0.0291623]\n",
      "[0.5499934]\n",
      "[-0.0291623]\n",
      "[0.5499938]\n",
      "[-0.0291623]\n",
      "[0.54999417]\n",
      "[-0.0291623]\n",
      "[0.54999452]\n",
      "[-0.0291623]\n",
      "[0.54999485]\n",
      "[-0.0291623]\n",
      "[0.54999516]\n",
      "[-0.0291623]\n",
      "[0.54999545]\n",
      "[-0.0291623]\n",
      "[0.54999572]\n",
      "[-0.0291623]\n",
      "[0.54999598]\n",
      "[-0.0291623]\n",
      "[0.54999622]\n",
      "[-0.0291623]\n",
      "[0.54999645]\n",
      "[-0.0291623]\n",
      "[0.54999666]\n",
      "[-0.0291623]\n",
      "[0.54999686]\n",
      "[-0.0291623]\n",
      "[0.54999705]\n",
      "[-0.0291623]\n",
      "[0.54999723]\n",
      "[-0.0291623]\n",
      "[0.54999739]\n",
      "[-0.0291623]\n",
      "[0.54999755]\n",
      "[-0.0291623]\n",
      "[0.5499977]\n",
      "[-0.0291623]\n",
      "[0.54999783]\n",
      "[-0.0291623]\n",
      "[0.54999796]\n",
      "[-0.0291623]\n",
      "[0.54999809]\n",
      "[-0.0291623]\n",
      "[0.5499982]\n",
      "[-0.0291623]\n",
      "[0.54999831]\n",
      "[-0.0291623]\n",
      "[0.54999841]\n",
      "[-0.0291623]\n",
      "[0.54999851]\n",
      "[-0.0291623]\n",
      "[0.5499986]\n",
      "[-0.0291623]\n",
      "[0.54999868]\n",
      "[-0.0291623]\n",
      "[0.54999876]\n",
      "[-0.0291623]\n",
      "[0.54999883]\n",
      "[-0.0291623]\n",
      "[0.5499989]\n",
      "[-0.0291623]\n",
      "[0.54999897]\n",
      "[-0.0291623]\n",
      "[0.54999903]\n",
      "[-0.0291623]\n",
      "[0.54999909]\n",
      "[-0.0291623]\n",
      "[0.54999914]\n",
      "[-0.0291623]\n",
      "[0.5499992]\n",
      "[-0.0291623]\n",
      "[0.54999924]\n",
      "[-0.0291623]\n",
      "[0.54999929]\n",
      "[-0.0291623]\n",
      "[0.54999933]\n",
      "[-0.0291623]\n",
      "[0.54999937]\n",
      "[-0.0291623]\n",
      "[0.54999941]\n",
      "[-0.0291623]\n",
      "[0.54999944]\n",
      "[-0.0291623]\n",
      "[0.54999948]\n",
      "[-0.0291623]\n",
      "[0.54999951]\n",
      "[-0.0291623]\n",
      "[0.54999954]\n",
      "[-0.0291623]\n",
      "[0.54999957]\n",
      "[-0.0291623]\n",
      "[0.54999959]\n",
      "[-0.0291623]\n",
      "[0.54999962]\n",
      "[-0.0291623]\n",
      "[0.54999964]\n",
      "[-0.0291623]\n",
      "[0.54999966]\n",
      "[-0.0291623]\n",
      "[0.54999968]\n",
      "[-0.0291623]\n",
      "[0.5499997]\n",
      "[-0.0291623]\n",
      "[0.54999972]\n",
      "[-0.0291623]\n",
      "[0.54999974]\n",
      "[-0.0291623]\n",
      "[0.54999975]\n",
      "[-0.0291623]\n",
      "[0.54999977]\n",
      "[-0.0291623]\n",
      "[0.54999978]\n",
      "[-0.0291623]\n",
      "[0.54999979]\n",
      "[-0.0291623]\n",
      "[0.54999981]\n",
      "[-0.0291623]\n",
      "[0.54999982]\n",
      "[-0.0291623]\n",
      "[0.54999983]\n",
      "[-0.0291623]\n",
      "[0.54999984]\n",
      "[-0.0291623]\n",
      "[0.54999985]\n",
      "[-0.0291623]\n",
      "[0.54999986]\n",
      "[-0.0291623]\n",
      "[0.54999987]\n",
      "[-0.0291623]\n",
      "[0.54999987]\n",
      "[-0.0291623]\n",
      "[0.54999988]\n",
      "[-0.0291623]\n",
      "[0.54999989]\n",
      "[-0.0291623]\n",
      "[0.5499999]\n",
      "[-0.0291623]\n",
      "[0.5499999]\n",
      "[-0.0291623]\n",
      "[0.54999991]\n",
      "[-0.0291623]\n",
      "[0.54999991]\n",
      "[-0.0291623]\n",
      "[0.54999992]\n",
      "[-0.0291623]\n",
      "[0.54999992]\n",
      "[-0.0291623]\n",
      "[0.54999993]\n",
      "[-0.0291623]\n",
      "[0.54999993]\n",
      "[-0.0291623]\n",
      "[0.54999994]\n",
      "[-0.0291623]\n",
      "[0.54999994]\n",
      "[-0.0291623]\n",
      "[0.54999994]\n",
      "[-0.0291623]\n",
      "[0.54999995]\n",
      "[-0.0291623]\n",
      "[0.54999995]\n",
      "[-0.0291623]\n",
      "[0.54999995]\n",
      "[-0.0291623]\n",
      "[0.54999996]\n",
      "[-0.0291623]\n",
      "[0.54999996]\n",
      "[-0.0291623]\n",
      "[0.54999996]\n",
      "[-0.0291623]\n",
      "[0.54999996]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999997]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999998]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.54999999]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[0.55]\n",
      "[-0.0291623]\n",
      "[[[0.55 0.55]\n",
      "  [0.55 0.55]]]\n"
     ]
    }
   ],
   "source": [
    "l = np.array([[[ 6, 4],\n",
    "              [ 9, 3]]]) / 10\n",
    "m = np.array([[[1, 2, 0, 1, 1, 1], \n",
    "              [2, 1, 1, 1, 2, 0],\n",
    "              [1, 1, 2, 0, 0, 1],\n",
    "              [2, 2, 0, 1, 1, 2],\n",
    "              [1, 0, 1, 2, 0, 1],\n",
    "              [0, 2, 1, 0, 1, 2]]]) / 10\n",
    "k1 = np.array([[ 1,  0, -1], \n",
    "               [ 0,  1, -1],\n",
    "               [-1, -1,  1]]) / 10\n",
    "k2 = np.array([[ 0, -1,  1], \n",
    "               [ 1,  0, -1],\n",
    "               [ 0, -1,  1]]) / 10\n",
    "\n",
    "neural_network = NeuralNetwork(m, l, 1, \n",
    "                              ConvolutionLayer(1, 3, ReLU, None, \"CL1\"),\n",
    "                              ConvolutionLayer(1, 3, ReLU, ReLU_deriv, \"CL2\"))\n",
    "neural_network.train(500, 0.15)\n",
    "neural_network.assess(m, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
